# DIFFERENZA TRA HIVE, IMPALA E SQOOP
(E TRA DATA WAREHOUSE E MOTORE SQL)

## CONCETTO FONDAMENTALE: GOVERNA VS POSSIEDE

**Storage** (HDFS, S3, ADLS)
- Possiede fisicamente i dati (file che contengono le righe effettive)
- Garantisce durabilità, replica e accesso distribuito
- Non conosce schemi, tabelle, strutture: vede solo byte e blocchi

**Data Warehouse** (Hive, Snowflake, BigQuery, Redshift)
Governa i metadati:
a) Organizza metadati: Ossia definisce schemi, tabelle, partizioni, sicurezza e processi ETL (Extract, trasform, load*)
b) Cataloga dove stanno i dati e come interpretarli
c) I file fisici rimangono nello storage sottostante
- Nota: alcuni DW cloud (Snowflake, BigQuery) integrano anche lo storage, ma il principio resta

*Extract (Estrazione): consiste nel prelevare dati da una o più fonti eterogenee (database, file, API, ecc.).
Transform (Trasformazione): i dati estratti vengono puliti, arricchiti, aggregati o convertiti in un formato adatto all’analisi o all’archiviazione. Questa fase può includere la normalizzazione, la deduplicazione, la conversione di tipi di dato, il calcolo di nuovi campi, ecc.
Load (Caricamento): i dati trasformati vengono caricati nel sistema di destinazione, tipicamente un data warehouse, un database analitico o un data lake, dove saranno disponibili per analisi, reportistica o altre applicazioni.

**Motori SQL** (Impala, Hive, Spark SQL, Trino)

Creano, modificano ed eliminano dati e strutture, tramite comandi SQL:

CREATE: crea tabelle, schemi, database, viste, indici (es. CREATE TABLE ...)
INSERT: aggiunge nuove righe (es. INSERT INTO ...)
UPDATE: modifica dati esistenti (es. UPDATE ... SET ...)
DELETE: elimina righe (es. DELETE FROM ...)
DROP: elimina tabelle, schemi, database (es. DROP TABLE ...)
ALTER: modifica la struttura (aggiunge/rimuove colonne, cambia tipi, ecc.)

## DATA LAKE VS DATA WAREHOUSE VS DATA CATALOG

| Aspetto                | Data Lake                   | Data Warehouse              | Data Catalog                              |
|------------------------|-----------------------------|-----------------------------|----------------------------------------   |
| Possiede/Governa       | Possiede i dati fisicamente | Governa metadati tecnici (non possiede i dati)| Governa metadati di business (non possiede i dati)|
| Funzione principale    | Storage (memorizza)         | Query + Governance tecnica  | Documenta + Governance business           |
| Esegue query SQL       | No (solo storage)           | Sì (SELECT, JOIN, ecc.)     | No (non esegue query)                     |
| Tipo dati              | Grezzi, strutturati e misti | Strutturati                 | Non applicabile (cataloga, non memorizza) |
| Schema                 | On-read (al momento della query)| On-write (alla scrittura)  | Documenta schemi da altri sistemi      |
| Dove stanno i metadati | N/A: ha solo file fisici  |  database relazionali esterni tramite il servizio Hive Metastore | DB del Catalog (es. Postgres/MySQL/Atlas+ES) |
| Governance tecnica     | Limitata o assente          | Elevata (schemi, permessi, ETL) | No (cataloga quella degli altri)      |
| Governance business    | Assente (serve Catalog)     | Assente (serve Catalog)     | Sì (ownership, policy, lineage, qualità)  |
| Utenti                 | Data scientist, ingegneri dati | Analisti, BI, business      | Data steward, governance, business     |
| Esempi                 | Hadoop, S3, ADLS            | Hive, Snowflake, BigQuery   | Alation, Collibra, Apache Atlas           |

**⚠️ Attenzione: Data Lake ≠ Data Catalog**

| Aspetto              | Data Lake                                    | Data Catalog                                      |
|----------------------|--------------------------------------------|------------------------------------------------     |
| **Cosa è**           | Storage system (memorizza dati fisicamente)| Metadata/Governance platform (documenta dati)       |
| **Possiede/Governa** | Possiede (memorizza file)                  | Governa metadati (non possiede dati; conserva i propri metadati in un DB interno) |
| **Contiene**         | Dati grezzi, strutturati e misti           | Definizioni business, ownership, policy, lineage    |
| **Esempi**           | Hadoop/HDFS, S3, ADLS                      | Alation, Collibra, Apache Atlas                     |
| **Funzione**         | Memorizzare dati durevolmente              | Catalogare dove stanno i dati e cosa significano    |
| **Conosce schemi**   | No (on-read)                               | Sì (documenta schemi e strutture)                   |
| **Esegue query**     | No (solo storage)                          | No (cataloga, non esegue query)                     |

### DIFFERENZA TRA GOVERNANCE E MANAGEMENT###

- Gestione (management): riguarda l’amministrazione operativa quotidiana di risorse, processi e attività. Si occupa di eseguire, monitorare e ottimizzare le operazioni.
- Governance: indica l’insieme di regole, politiche, responsabilità e controlli che definiscono come vengono prese le decisioni, chi è responsabile, come si garantisce la conformità e la qualità, e come si gestiscono rischi e sicurezza.

### ANALOGIA CON IL SUPERMERCATO (PRATICO E CHIARO)

**Data Lake = Mercato all'ingrosso (disordinato)**
- Verdure sporche appena raccolte
- Carni non ancora pulite  
- Ingredienti di tutti i tipi ammassati insieme
- Nessuna organizzazione, tutto mischiato
- Informazioni incomplete o duplicate
- **Concetto**: dati grezzi, senza struttura, come arrivano dalle sorgenti

**Data Warehouse = Ristorante pulito e organizzato**
- Verdure lavate, tagliate e organizzate
- Carni già porzionate e etichettate
- Tutto catalogato negli scaffali giusti (reparto verdure, carni, latticini, ecc.)
- Pronto per essere cucinato/usato
- Schema rigoroso, dati puliti e strutturati
- **Concetto**: dati trasformati, pronti per l'analisi, con schemi definiti

**Data Catalog = depliant/menu del Ristorante**
- Ti dice: "Il tonno fresco è nel reparto pesce, arrivato stamattina, biologico"
- Ti dice: "Il latte ha scadenza tra 5 giorni, contiene lattosio"
- Non stai mangiando lì, stai solo **leggendo informazioni su cosa c'è**
- Documenta significato, proprietario, regole, qualità
- **Concetto**: metadati di business, non i dati effettivi

Nell’analogia:

- Il Data Catalog è come il depliant/menu del ristorante: ti dice dove si trova ogni ingrediente, a cosa serve, chi lo gestisce, ecc. (ma non contiene la verdura stessa).
- Il Data Warehouse è la cucina/dispensa dove la verdura è effettivamente organizzata, pulita e pronta per essere usata.


### ESEMPIO PRATICO: RICERCA DI DATI SU VENDITE

**Scenario**: "Quali sono i clienti che hanno speso più di 1000€?"

**Step 1: Consulti il Data Catalog** (per scoprire dove trovare i dati)
- Cerchi: "Dove trovo le spese dei clienti?"
- Risposta dal Catalog: 
  - ✅ Esiste una tabella chiamata `ORDINI`
  - ✅ Ha le colonne: `id_cliente`, `importo`, `data`
  - ✅ È nel Data Warehouse (tabella strutturata)
  - ✅ È aggiornata ogni 6 ore
  - ✅ Proprietario: Team Finance

**Step 2: Interroghi il Data Warehouse** (per ottenere i dati veri)
```sql
SELECT id_cliente, SUM(importo) as spesa_totale
FROM ORDINI
WHERE SUM(importo) > 1000
GROUP BY id_cliente;
```
- Risultato: lista effettiva di clienti e loro spese (righe reali)

**NON ricerchi la spesa nel Data Catalog**, ricerchi il **NOME E LA POSIZIONE** della tabella nel Catalog, poi interroghi il Warehouse per i dati effettivi.

**⚠️ Data Catalog NON è un contenitore**

Il Data Catalog **non memorizza dati**, non è un contenitore:
- **Non contiene** dati effettivi (righe, file, tabelle)
- **Non contiene** metadati tecnici (se non come riferimenti/indici)
- **Documenta e cataloga** i dati/tabelle che vivono in altri sistemi (Data Warehouse, Storage, Database)
- **Punta a** e aggrega informazioni da altre piattaforme
66
**⚠️ Data Catalog ≠ Data Warehouse**

| Aspetto              | Data Warehouse (Hive, Snowflake, BigQuery) | Data Catalog (Alation, Collibra, Atlas) |
|----------------------|--------------------------------------------|-----------------------------------------|
| **Esegue query SQL** | Sì: SELECT, INSERT, UPDATE, DELETE         | No: non esegue query                    |
| **Governa dati**     | Sì: schemi, tabelle, partizioni, permessi  | No: cataloga informazioni da altri sistemi |
| **Memorizza dati**   | No (governa, non possiede); dati in storage | No (non memorizza dati, solo metadati business) |
| **Controlla accesso**| Sì: RBAC/ABAC, masking, filtri riga/colonna | No: eventuali preview passano per il motore |
| **Metadati**         | Tecnici: information_schema, statistiche, indici | Business: significato, ownership, policy, lineage |
| **Utenti**           | Ingegneri, analisti, BI (query e trasformazioni) | Data steward, governance, business (ricerca significato) |
| **Esempio di query** | `SELECT * FROM vendite WHERE anno=2024` → righe reali | `search("vendite", filter="PII")` → metadati business |

**In una frase:**
- **Data Warehouse** = motore che esegue query e governa la struttura tecnica (schemi, permessi SQL)
- **Data Catalog** = libreria che documenta il significato business e la governance (non possiede né interroga dati)

**Dove vivono fisicamente i metadati?**

I metadati del Data Catalog si trovano **nel database interno del Catalog stesso**, non nei sistemi che cataloga:

```
┌─────────────────────────┐
│ Storage (S3, HDFS)      │
│ file_vendite.parquet    │
└─────────────────────────┘
           ↑
           
┌──────────────────────────────┐
│ Data Warehouse (Hive)        │
│ TABLE vendite (schema)       │
│ Hive Metastore (metadati)    │
└──────────────────────────────┘
           ↑
           
┌────────────────────────────────────────┐
│ Data Catalog (Alation/Collibra/Atlas)  │
│ ┌──────────────────────────────────┐   │
│ │ Database interno del Catalog:    │   │
│ │ - "vendite = reddito lordo"      │   │
│ │ - "owner: Mario Rossi"           │   │
│ │ - "sensibile GDPR"               │   │
│ │ - "usata in 5 report"            │   │
│ │ - "lineage: Salesforce→Hive"     │   │
│ └──────────────────────────────────┘   │
└────────────────────────────────────────┘
```

## Esempio di come può apparire una scheda di Data Catalog per una tabella di database con 10 colonne

Tabella: clienti

Descrizione: Anagrafica clienti attivi

Owner: Team Marketing

Aggiornamento: Giornaliero

Colonne:

Nome colonna	Tipo dato	Descrizione	Sensibilità	Esempio
id_cliente	INT	Identificativo univoco cliente	Bassa	12345
nome	VARCHAR	Nome del cliente	Bassa	Mario
cognome	VARCHAR	Cognome del cliente	Bassa	Rossi
email	VARCHAR	Email di contatto	Alta (GDPR)	mario@esempio.it
telefono	VARCHAR	Numero di telefono	Alta	3331234567
data_nascita	DATE	Data di nascita	Media	1980-01-01
indirizzo	VARCHAR	Indirizzo di residenza	Media	Via Roma 1
città	VARCHAR	Città di residenza	Bassa	Milano
paese	VARCHAR	Paese di residenza	Bassa	Italia
data_registrazione	DATE	Data di registrazione nel sistema	Bassa	2020-05-10
Policy GDPR: Email e telefono sono dati sensibili, accesso limitato.

Lineage: Origine dati: CRM → ETL → Database clienti

Usato in: Report mensili, dashboard marketing

**Di seguito i Datacatalog/governance e relativo DB:**
 
- **Alation (Data Catalog)**: usa database relazionale interno (tipicamente PostgreSQL; supporta MySQL/Oracle) + repository per documenti/descrizioni
- **Collibra (Data Catalog)**: usa database interno (PostgreSQL) + repository metadati centralizzato
- **Apache Atlas (Data Catalog)**: usa HBase (per metadati, NoSQL a colonne) + Elasticsearch (per ricerca) nel cluster Hadoop

**Caratteristica importante: indipendenza**
- Se il Data Warehouse (Hive) cade, il Data Catalog rimane online (documenta comunque cosa c'era)
- Se il Data Catalog cade, i dati rimangono intatti nel Storage/DW (il Catalog non li controlla, solo li documenta)
- I metadati del Catalog sono **completamente separati** e indipendenti dall'architettura dati


## DATABASE RELAZIONALE VS NON RELAZIONALE VS TRANSAZIONALE (RIASSUNTO)

| Aspetto            | Relazionale (RDBMS)                                 | Non relazionale (NoSQL)                                        | Transazionale (ACID)                                               |
|--------------------|------------------------------------------------------|----------------------------------------------------------------|---------------------------------------------------------------------|
| Modello dati       | Tabelle, righe, colonne                             | Documenti JSON, key-value, wide-column, grafo                  | Qualsiasi modello che offra ACID                                    |
| Schema             | Rigoroso (schema-on-write)                          | Flessibile/schema-on-read, spesso denormalizzato               | Rigoroso dove serve coerenza transazionale                          |
| Transazioni        | ACID native (commit/rollback)                       | Variabile: spesso eventual consistency; alcune piattaforme ACID | Focus su atomicità/isolamento/durabilità                            |
| Scalabilità        | Tipicamente verticale (sharding possibile)          | Orizzontale nativa                                             | Dipende dal motore: RDBMS scalano meno orizzontalmente; alcuni NewSQL/NoSQL ACID scalano orizzontalmente |
| Casi d’uso tipici  | OLTP classico: ordini, fatture, anagrafiche, ERP    | Contenuti web/mobile, log/IoT, cataloghi prodotti, time-series, grafi | Qualsiasi caso con requisiti di scrittura atomica (pagamenti, ordini, inventory) |
| Esempi             | PostgreSQL, MySQL, Oracle, SQL Server               | MongoDB (documenti), Redis (KV), Cassandra/HBase (wide-column), Neo4j (grafo) | PostgreSQL/MySQL/Oracle/SQL Server; Google Spanner, CockroachDB, Yugabyte; MongoDB transazioni multi-doc |

In sintesi:
- **Relazionale** = schema rigido, JOIN forti, ACID nativo.
- **Non relazionale** = schema flessibile, denormalizzazione, scala orizzontale facile, coerenza configurabile.
- **Transazionale (ACID)** = requisito di atomicità e coerenza: può essere RDBMS o alcune piattaforme NoSQL/NewSQL che offrono transazioni.

## DATI, METADATI E GOVERNANCE

**Dati (Righe)**
- Contenuto informativo vero: righe 
- Memorizzati nei file (Parquet, ORC, CSV, ecc.) su storage
- Vivono in: HDFS, S3, ADLS (mai in Hive Metastore né in Impala)

**Metadati**
- Informazioni che descrivono i dati
- Schema, tabelle, colonne, tipi, partizioni, percorsi, permessi, statistiche
- Nota: la "tabella" come oggetto (nome, schema, proprietà, location, formati) è metadato; le righe contenute sono dati.
In altre parole, quando crei una tabella in Hive o Impala, stai solo registrando nel Metastore un insieme di informazioni che descrivono come interpretare i dati fisici (ad esempio, dove si trovano i file, quali colonne aspettarsi, i tipi di dato, eventuali partizioni e i formati di storage). Questi metadati permettono ai motori SQL di sapere come leggere e scrivere i dati, ma i dati veri e propri (le righe) risiedono nei file su HDFS, S3 o altri storage. Se elimini la tabella dal Metastore, i file fisici possono anche rimanere intatti: hai solo perso la "mappa" che li descriveva, non i dati stessi.
- Gestiti da: Hive Metastore (per Hive/Impala)

### SQL vs Data Catalog (Riassunto veloce)

| Aspetto         | SQL (motore)                                               | Data Catalog (Alation/Collibra/Atlas)                          |
|-----------------|-------------------------------------------------------------|-----------------------------------------------------------------|
| Scopo           | Leggere/scrivere dati, eseguire query, gestire schema       | Documentare significato, ownership, policy, lineage             |
| Gestisce        | Tabelle, colonne, tipi, DDL/DML, permessi (GRANT/REVOKE)    | Definizioni business, KPI, classificazioni (PII), qualità, glossario |
| Enforcement     | Sì: controlli d’accesso, masking, filtri a livello riga/colonna | No: non esegue query; eventuali preview rispettano i permessi della sorgente |
| Preview dati    | Nativo (via query)                                          | Facoltativo; passa tramite la connessione al motore             |
| Dove vive       | Motore dati (Hive/Impala/Snowflake/PostgreSQL/…)            | Piattaforma separata                                            |
| Utenti          | Ingegneri, analisti, BI                                     | Data steward, governance, business                              |
| Esempi          | Hive, Impala, Spark SQL, Snowflake, BigQuery                | Alation, Collibra, Apache Atlas                                 |

In una frase: SQL governa struttura e accesso; il Catalog governa significato e regole.

**Gerarchia organizzativa: Database > Schema > Tabella**

```
DATABASE (contenitore generale)
│
├── SCHEMA (gruppo logico / namespace)
│   │
│   └── TABELLA (struttura dati con righe/colonne)
│       │
│       ├── COLONNA (campo: nome + tipo)
│       └── RIGA (record singolo)
```

**Schema (o Database in Hive)** = Contenitore/Namespace
- Raggruppa tabelle correlate logicamente
- Evita conflitti di nome (es. `finance.vendite` vs `marketing.vendite`)
- Gestisce permessi a livello di gruppo

**Tabella** = Struttura dati effettiva
- Contiene righe (record) e colonne (campi)
- Ha schema definito (nomi colonne + tipi di dato)

**Esempio pratico:**
```sql
-- Creare uno schema (in Hive si chiama DATABASE)
CREATE DATABASE finance;
CREATE DATABASE marketing;

-- Creare tabelle in schemi diversi
CREATE TABLE finance.vendite (id INT, importo DECIMAL, data DATE);
CREATE TABLE marketing.vendite (id INT, prodotto VARCHAR, campagna VARCHAR);
-- ↑ Stesso nome "vendite", ma tabelle diverse!

-- Accedere alle tabelle
SELECT * FROM finance.vendite;      -- Tabella del team Finance
SELECT * FROM marketing.vendite;    -- Tabella diversa del team Marketing
```

**Analogia rapida:**
```
Database = Biblioteca intera
Schema = Piano della biblioteca (es. Piano 1: Narrativa, Piano 2: Tecnica)
Tabella = Scaffale con libri (es. Romanzi, Poesia, Informatica)
```

**Governance del Data Warehouse** (governance TECNICA)

Il Data Warehouse gestisce la governance **tecnica**, ma non quella **di business**:

✅ **Governance TECNICA** (gestita dal Data Warehouse):
- Schemi, tabelle, partizioni e tipi di dato
- Sicurezza di accesso: permessi SQL (GRANT/REVOKE su tabelle)
- Storico tecnico: versioni, snapshot (con Delta/Iceberg)
- Processi ETL: orchestrazione di trasformazioni e caricamenti
- Metadati tecnici: formati (Parquet/ORC), paths, statistiche

❌ **Governance DI BUSINESS** (serve un Data Catalog esterno):
- Significato aziendale: cosa rappresenta ogni campo per l'azienda
- Ownership: chi è il data owner, chi è responsabile
- Policy aziendali: GDPR, retention, classificazione (pubblico/riservato)
- Lineage completo: provenienza e tutte le trasformazioni subite
- Qualità: anomalie, duplicati, completezza, validità
- Strumenti: Alation, Collibra, Apache Atlas

**Esempio pratico:**
```sql
-- Data Warehouse gestisce (governance tecnica):
CREATE TABLE vendite (id INT, importo DECIMAL, data DATE);
GRANT SELECT ON vendite TO utente_finance;
-- ✅ Sa: schema, permessi SQL

-- Data Catalog gestisce (governance business):
"vendite.importo = reddito lordo mensile (include bonus)"
"Owner: Mario Rossi (CFO), sensibile GDPR, retention 7 anni"
-- ✅ Sa: significato, policy, ownership
```

**Accesso a dati sensibili (GDPR): Alation vs SQL**
- **Alation** è un Data Catalog: non memorizza né elabora i dati; mo

- In Hadoop (Hive/Impala) usare **Apache Ranger** per:
  - Column masking (es. mascherare `email` o offuscare parzialmente `codice_fiscale`)
  - Row filter (limitare righe visibili per reparto/paese)
  - Audit centralizzato delle query
- Alternative/varianti: **Sentry** (legacy), **Lake Formation** (AWS), **Unity Catalog** (Databricks) con tag/policy.
- Per GDPR (diritto all’oblio/retention): usare **Delta/Iceberg/Hudi** per `DELETE` e gestione snapshot/`VACUUM`.

**Esempi pratici: masking e filtri (GDPR)**
- **Ranger – Column Masking**: imposta una policy sulla colonna `email` di `finance.vendite` (azione SELECT) con mascheramento parziale (es. mostra solo i primi 3 caratteri e il dominio) o totale (NULL/hash). La maschera si applica a tutte le query, viste incluse.
- **Ranger – Row-level Filter**: definisci un filtro di riga, ad es. `cntry_cd = 'IT'` per il ruolo `FINANCE_IT`, oppure usa attributi utente (es. `${USER.country}`) per filtri dinamici per paese/reparto.
- **Ranger + Atlas (Tag-based)**: tagga in Atlas le colonne PII (es. `email`, `codice_fiscale`) e crea in Ranger una policy “per tag” che applica maschere/filtri automaticamente a tutte le tabelle con quel tag.

*Pattern SQL-only (viste sicure), se non hai Ranger:*
```sql
-- Mascheramento colonna email tramite vista (Hive/Impala)
CREATE SCHEMA IF NOT EXISTS secure;
CREATE OR REPLACE VIEW secure.vendite_masked AS
SELECT
  id,
  CONCAT(SUBSTR(email, 1, 3), '***', SUBSTR(email, LOCATE('@', email))) AS email_mascherata,
  data,
  cntry_cd
FROM finance.vendite;

-- Filtro per righe (versione statica)
CREATE OR REPLACE VIEW secure.vendite_it AS
SELECT * FROM finance.vendite WHERE cntry_cd = 'IT';

-- Filtro per righe basato sull'utente (mappa utente→paese)
-- Nota: in Hive usa CURRENT_USER(); in Impala la funzione è USER()
CREATE TABLE IF NOT EXISTS security.user_country (utente STRING, cntry_cd STRING);
CREATE OR REPLACE VIEW secure.vendite_per_utente AS
SELECT v.*
FROM finance.vendite v
JOIN security.user_country m
  ON m.utente = CURRENT_USER()
 AND v.cntry_cd = m.cntry_cd;
```

Suggerimenti operativi:
- Preferisci policy centralizzate (Ranger/Lake Formation/Unity Catalog) a logica applicativa nelle viste: sono auditate e coerenti.
- Abilita audit di query e usa cifratura a riposo/in transito (HDFS Transparent Encryption, S3/KMS, TLS).
- Per GDPR “right to be forgotten”, combina `DELETE` su formati transazionali (Delta/Iceberg/Hudi) con procedure di compattazione/`VACUUM` secondo le policy di retention.

**Cosa SI trova con SQL** (il database conosce)

*I dati effettivi*
```sql
SELECT nome, importo FROM vendite WHERE anno = 2024;
-- Restituisce: Ana | 150€, Marco | 200€, Sofia | 300€, ...
-- SQL accede ai dati fisici e li legge
```

*Metadati tecnici* (il database conosce la sua struttura)
```sql
-- Nome colonne e tipi (Hive/Impala)
DESCRIBE vendite;
-- Oppure più dettagliato:
DESCRIBE EXTENDED vendite;
-- Risultato: nome_colonna | tipo_dato | commento
-- importo (DECIMAL), data (DATE), nome (VARCHAR), ecc.

-- Elencare tutte le colonne con formato dettagliato
SHOW COLUMNS FROM vendite;
-- Risultato: column_name, data_type, comment

-- Struttura completa della tabella (DDL)
SHOW CREATE TABLE vendite;
-- Risultato: lo statement CREATE TABLE completo con location, formato, partizioni

-- Nota: information_schema esiste in DB relazionali (MySQL, PostgreSQL)
-- ma Hive/Impala usano DESCRIBE, SHOW COLUMNS, SHOW CREATE TABLE
```

**Cosa NON si trova con SQL** (serve un Data Catalog esterno)

*Metadati di business* (il significato aziendale dei dati)
- "La colonna `importo` rappresenta il **reddito lordo annuale** (comprende bonus e incentivi)"
- "Il KPI `conversion_rate` è calcolato come **(ordini / visitatori) * 100**"
- "La tabella `vendite` è **proprietà del team Finance** e curata da Mario Rossi (data owner)"
- "Questo dato è **sensibile: GDPR**, accesso limitato al solo team Finance e Compliance"

*Governance e compliance* (regole aziendali e tracciabilità)
- **Lineage**: "Da dove viene questo dato? Salesforce → ETL → Hadoop → report finale (con 3 trasformazioni)"
- **Qualità**: "Questo dato ha problemi? 5% di valori duplicati, 2% di null anomali, 1 outlier rilevato"
- **Audit trail**: "Chi ha modificato questa colonna? (storico completo di cambiamenti e responsabili)"
- **Retention policy**: "Quanto tempo conservo questi dati? Cancellare dopo 5 anni (GDPR 'right to be forgotten')"
- **Classificazione**: "Questo è pubblico, ristretto, sensibile o strettamente confidenziale?"

*Strumenti specializzati* (dove mettere questi metadati)
- **Alation**: Catalogo di business - documenta significato, ownership, KPI, criticità
- **Collibra**: Governance platform - policy complete, compliance, audit trail
- **Apache Atlas**: Metadata repository - lineage, dipendenze, trasformazioni (open-source)

**Perché SQL non può contenere questi dati?**
- SQL è disegnato per dati strutturati (tabelle, righe, colonne) e query transazionali
- I metadati di business sono semantica e regole: difficili da organizzare in tabelle SQL
- Cambian frequentemente: le regole aziendali si aggiornano, ma il database rimane stabile
- Gestiti da team diversi: DBA gestisce il DB, Data Governance gestisce le policy

**Esempio completo: una colonna `email`**
```
NEL DATABASE (SQL):
├─ Data type: VARCHAR(255)
├─ Is nullable: YES
├─ Primary Key: NO
└─ Foreign Key: NO

NEL DATA CATALOG (Alation/Collibra):
├─ Proprietario (Data Owner): Antonio Bianchi (Marketing)
├─ Significato: Email di contatto del cliente per comunicazioni di marketing
├─ GDPR sensitivity: SENSIBILE (diritto all'oblio, consenso richiesto)
├─ Qualità: 97% filled, 0.5% duplicati, valido come email
├─ Lineage: Salesforce → Sqoop → HDFS → Hive table → Report BI
├─ Retention: Cancellare dopo 3 anni di inattività (GDPR compliance)
├─ Usato in: 12 report BI, 3 model ML, Dashboard Executive
└─ Ultimo aggiornamento: 2 ore fa (batch notturno da Salesforce)
```

**Riassunto finale**

| Aspetto | Dove | Chi lo gestisce |
|---------|------|-----------------|
| **Dati effettivi** | Database (SELECT) | Developers, analisti |
| **Metadati tecnici** (schema, tipi, indici) | Database (information_schema) | DBA, SQL |
| **Metadati di business** (significato, ownership) | Data Catalog | Data Steward, Domain Expert |
| **Governance** (GDPR, lineage, audit, qualità) | Data Governance Platform | Compliance Officer, Data Governance |

**In una frase**: SQL gestisce la **struttura e i contenuti**, il Data Catalog gestisce il **significato e le regole aziendali**.

---

## MODELLI DI ESECUZIONE: BATCH VS INTERATTIVO

**Esecuzione Batch (Hive)**
- Ogni query scatena più fasi (map/shuffle/reduce) con startup di container YARN
- Overhead di avvio: secondi/decine di secondi per fase
- Dati letti da HDFS in blocchi, spesso riscritti su disco tra fasi (spill/shuffle)
- Fault tolerance: task falliti si riavviano automaticamente
- Latenza: secondi/minuti su grandi volumi
- Throughput: massimo, parallelizzazione su molti nodi
- Ideale per: ETL pesanti, elaborazioni notturne, grandi join/aggregazioni con shuffle massivi

**Esecuzione Interattiva (Impala)**
- In-memory, no MapReduce, motore MPP (Massively Parallel Processing)
- Long-running: i daemon Impala rimangono accesi, nessun startup YARN per query
- Latenza: secondi o millisecondi
- Limits: dati devono stare in RAM totale del cluster
- Non robusto per: job lunghi, riscritture massive, fault recovery
- Ideale per: BI, dashboard, analisi esplorativa, query puntuali

**Confronto rapido**
| Aspetto           | Hive (Batch)              | Impala (Interattivo)         |
|-------------------|---------------------------|------------------------------|
| Esecuzione        | MapReduce/Tez/Spark       | In-memory MPP                |
| Latenza           | Secondi/minuti            | Secondi/millisecondi         |
| Startup YARN      | Sì, overhead               | No, daemon long-running     |
| Throughput        | Massimo, altamente scalabile | Minore, limitato da RAM   |
| Fault tolerance   | Ottima (retry task)       | Scarsa (fine query)          |
| Adatto per        | ETL, batch notturni       | BI, analisi interattiva      |
| Non adatto per    | Query real-time / BI rapido | ETL pesanti, volumi enormi |

**One-liner**: Usa **Hive per trasformazioni pesanti**, **Impala per letture veloci**.

---


# COME FUNZIONA MAPREDUCE

## 1. Introduzione

MapReduce è un framework di programmazione distribuita introdotto per risolvere in modo sistematico il problema dell’elaborazione di **dataset di dimensioni tali da non poter essere gestiti da un singolo nodo**. Il suo obiettivo principale non è la velocità di risposta, bensì la **scalabilità orizzontale**, la **robustezza ai guasti** e la **semplicità del modello di programmazione**.

Storicamente, MapReduce nasce per affrontare contesti caratterizzati da:

* grandi volumi di dati (terabyte o petabyte)
* hardware non affidabile (commodity hardware)
* necessità di elaborazioni batch ripetibili e deterministiche

Il paradigma accetta deliberatamente una maggiore latenza in cambio della capacità di:

* suddividere automaticamente il lavoro
* recuperare da errori di nodo o di processo
* garantire risultati consistenti anche in presenza di failure parziali

MapReduce non va quindi interpretato come un semplice modello astratto, ma come una **architettura di esecuzione completa**, profondamente integrata con il filesystem distribuito e il resource manager del cluster.

---

## 1.1 Principi di progettazione

Alla base di MapReduce vi sono alcuni principi architetturali chiave:

* **Move computation to data**: il codice viene portato dove risiedono i dati, non viceversa
* **Materializzazione su disco**: ogni fase critica produce output persistente
* **Stateless computation**: i task non mantengono stato condiviso
* **Determinismo**: a parità di input, l’output è sempre lo stesso

Questi principi spiegano sia i punti di forza sia i limiti del modello.
MapReduce è un framework di programmazione distribuita progettato per l’elaborazione batch di grandi volumi di dati su cluster di calcolo. Nasce con l’obiettivo di garantire **scalabilità orizzontale**, **fault tolerance** e **throughput elevato**, accettando come compromesso una **latenza elevata**.

Il paradigma si basa sull’idea di suddividere un problema complesso in due operazioni fondamentali:

* **Map**, che trasforma e proietta i dati
* **Reduce**, che aggrega i risultati intermedi

Tra queste due fasi avviene una fase cruciale detta **Shuffle & Sort**, responsabile del raggruppamento dei dati per chiave.

---

## 2. Architettura generale

MapReduce opera tipicamente sopra un filesystem distribuito (come HDFS) e un resource manager (come YARN). Il flusso logico è il seguente:

1. I dati di input sono memorizzati in file distribuiti e suddivisi in blocchi
2. Ogni blocco viene elaborato da un task di tipo Map
3. I risultati intermedi vengono ordinati e ridistribuiti tra i nodi
4. I task Reduce producono l’output finale

L’intera esecuzione è coordinata dal framework, che gestisce la pianificazione dei task, il riavvio in caso di errore e la località dei dati.

---

## 3. Parallelizzazione in MapReduce

### Chiarimento fondamentale: mapper e nodi

È essenziale distinguere tra **nodo** e **mapper**, poiché questa è una delle confusioni più comuni nello studio di MapReduce.

* Un **nodo** è una macchina fisica o virtuale del cluster (worker node).
* Un **mapper** è un **task di calcolo**, ovvero un processo eseguito su un nodo.

I mapper **non coincidono con i nodi**: un singolo nodo può eseguire più mapper in parallelo, mentre ogni mapper è associato a uno specifico split di input.

Questa distinzione è cruciale per comprendere il meccanismo di parallelizzazione e di fault tolerance di MapReduce.

Uno stesso cluster può quindi avere:

* pochi nodi fisici
* molti mapper logici

Il grado di parallelismo non dipende direttamente dal numero di nodi, ma dal numero di task Map eseguibili contemporaneamente, che a sua volta dipende dal numero di split e dalle risorse disponibili su ciascun nodo.

---

Uno degli aspetti centrali di MapReduce è la **parallelizzazione automatica** dell’elaborazione. Il programmatore non deve gestire thread, processi o sincronizzazione: è il framework che si occupa di suddividere il lavoro e distribuirlo sul cluster.

La parallelizzazione in MapReduce si manifesta su più livelli:

* **Parallelismo sui dati** (data parallelism)
* **Parallelismo sui task** (task parallelism)
* **Parallelismo tra fasi** (limitato e sequenziale)

### 3.1 Parallelismo sui dati

Il parallelismo principale deriva dalla suddivisione dei dati di input in **split indipendenti**. Ogni split può essere elaborato da un Mapper distinto.

Se un file è suddiviso in N split, il framework può lanciare fino a N Mapper in parallelo, compatibilmente con le risorse del cluster.

È importante sottolineare che:

* uno split logico non coincide necessariamente con un blocco HDFS
* uno stesso nodo può eseguire più mapper contemporaneamente

Questo modello consente una scalabilità quasi lineare aumentando il numero di nodi.

### 3.2 Parallelismo dei Mapper

Ogni Mapper:

* opera in modo completamente indipendente
* non condivide stato con altri Mapper
* può essere riavviato senza effetti collaterali

Questa indipendenza rende il modello altamente parallelo e tollerante ai guasti.

### 3.3 Parallelismo nella fase Reduce

Anche la fase Reduce è parallelizzabile. Il numero di Reducer è configurabile e determina il grado di parallelismo dell’aggregazione.

Ogni Reducer:

* riceve un sottoinsieme disgiunto delle chiavi
* elabora le chiavi in modo sequenziale

Il parallelismo totale nella fase Reduce è quindi limitato dal numero di reducer configurati.

### 3.4 Parallelizzazione della fase Shuffle & Sort

Anche la fase di **Shuffle & Sort** presenta elementi di parallelizzazione, ma con caratteristiche differenti rispetto alla fase Map.

È importante chiarire che lo Shuffle non è una singola operazione centralizzata, bensì un insieme di operazioni distribuite che coinvolgono mapper e reducer.

#### Parallelismo lato Mapper (map-side shuffle)

Ogni Mapper, in modo indipendente e in parallelo rispetto agli altri:

* partiziona il proprio output in base ai reducer
* ordina le coppie chiave-valore per chiave
* scrive i risultati su disco locale

Queste operazioni sono completamente parallelizzabili, poiché ciascun mapper lavora sul proprio output senza dipendere dagli altri.

#### Parallelismo lato Reducer (reduce-side shuffle)

Anche i Reducer operano in parallelo tra loro. Ogni reducer:

* contatta tutti i mapper
* scarica esclusivamente la propria partizione di dati
* esegue operazioni di merge e sort locali

Il trasferimento dei dati avviene quindi secondo uno schema **many-to-many**, che genera un intenso traffico di rete.

#### Barriera di sincronizzazione

Nonostante il parallelismo locale, la fase Shuffle & Sort introduce una **barriera globale di sincronizzazione**:

* la fase Reduce non può iniziare finché tutti i Mapper non hanno completato l’esecuzione
* la presenza di mapper lenti (stragglers) rallenta l’intero job

Per questo motivo, pur essendo parallelizzabile, lo Shuffle rappresenta il principale collo di bottiglia di MapReduce.

Il modello di esecuzione risulta quindi **massivamente parallelo**, ma **sincronizzato a fasi**.

---

### 3.5 Limiti della parallelizzazione

Nonostante l’elevato grado di parallelismo, MapReduce presenta limiti strutturali:

* separazione rigida tra le fasi Map e Reduce
* sincronizzazione globale durante lo Shuffle
* problemi di sbilanciamento delle chiavi (data skew)
* impossibilità di avviare i reducer prima del completamento di tutti i mapper

Questi limiti spiegano perché MapReduce sia efficiente per carichi batch di grandi dimensioni, ma poco adatto a elaborazioni iterative o interattive.

Nonostante l’elevato parallelismo locale, MapReduce presenta limiti strutturali:

* la fase Reduce non può iniziare finché la fase Map non è completata
* lo Shuffle introduce una barriera globale di sincronizzazione
* chiavi molto frequenti possono creare colli di bottiglia (data skew)

Il modello è quindi **massivamente parallelo**, ma **sincronizzato a fasi**.

Uno degli aspetti centrali di MapReduce è la **parallelizzazione automatica** dell’elaborazione. Il programmatore non deve gestire thread, processi o sincronizzazione: è il framework che si occupa di suddividere il lavoro e distribuirlo sul cluster.

La parallelizzazione in MapReduce si manifesta su più livelli:

* **Parallelismo sui dati** (data parallelism)
* **Parallelismo sui task** (task parallelism)
* **Parallelismo tra fasi** (limitato e sequenziale)

### 3.1 Parallelismo sui dati

Il parallelismo principale deriva dalla suddivisione dei dati di input in **split indipendenti**. Ogni split può essere elaborato da un Mapper distinto.

Se un file è suddiviso in N split, il framework può lanciare fino a N Mapper in parallelo, compatibilmente con le risorse del cluster.

È importante sottolineare che:

* uno split logico non coincide necessariamente con un blocco HDFS
* uno stesso nodo può eseguire più mapper contemporaneamente

Questo modello consente una scalabilità quasi lineare aumentando il numero di nodi.

### 3.2 Parallelismo dei Mapper

Ogni Mapper:

* opera in modo completamente indipendente
* non condivide stato con altri Mapper
* può essere riavviato senza effetti collaterali

Questa indipendenza rende il modello altamente parallelo e tollerante ai guasti.

### 3.3 Parallelismo nella fase Reduce

Anche la fase Reduce è parallelizzabile. Il numero di Reducer è configurabile e determina il grado di parallelismo dell’aggregazione.

Ogni Reducer:

* riceve un sottoinsieme disgiunto delle chiavi
* elabora le chiavi in modo sequenziale

Il parallelismo totale nella fase Reduce è quindi limitato dal numero di reducer configurati.

### 3.4 Limiti della parallelizzazione

Nonostante l’elevato parallelismo locale, MapReduce presenta limiti strutturali:

* la fase Reduce non può iniziare finché la fase Map non è completata
* lo Shuffle introduce una barriera globale di sincronizzazione
* chiavi molto frequenti possono creare colli di bottiglia (data skew)

Il modello è quindi **massivamente parallelo**, ma **sincronizzato a fasi**.

---

## 4. Fase MAP

### 4.1 Input e split

I file di input vengono suddivisi in **split logici**, generalmente allineati ai blocchi del filesystem distribuito (128 o 256 MB). Ogni split è assegnato a un Mapper.

### 4.2 Funzionamento del Mapper

Il Mapper:

* legge i record di input tramite un RecordReader
* applica una funzione definita dall’utente
* produce coppie chiave-valore intermedie

Esempio (conteggio delle parole):

Input:

```
apple banana apple cherry
```

Output del Mapper:

```
(apple, 1)
(banana, 1)
(apple, 1)
(cherry, 1)
```

### 4.3 Scrittura su disco locale

Le coppie chiave-valore prodotte non vengono immediatamente inviate in rete. Vengono prima:

* mantenute in un buffer in memoria
* ordinate per chiave
* scritte su disco locale (spill)

Questa scelta è fondamentale per la tolleranza ai guasti, ma introduce latenza.

### 3.1 Input e split

I file di input vengono suddivisi in **split logici**, generalmente allineati ai blocchi del filesystem distribuito (128 o 256 MB). Ogni split è assegnato a un Mapper.

### 3.2 Funzionamento del Mapper

Il Mapper:

* legge i record di input tramite un RecordReader
* applica una funzione definita dall’utente
* produce coppie chiave-valore intermedie

Esempio (conteggio delle parole):

Input:

```
apple banana apple cherry
```

Output del Mapper:

```
(apple, 1)
(banana, 1)
(apple, 1)
(cherry, 1)
```

### 3.3 Scrittura su disco locale

Le coppie chiave-valore prodotte non vengono immediatamente inviate in rete. Vengono prima:

* mantenute in un buffer in memoria
* ordinate per chiave
* scritte su disco locale (spill)

Questa scelta è fondamentale per la tolleranza ai guasti, ma introduce latenza.

---

## 4. Fase SHUFFLE & SORT

La fase di Shuffle & Sort rappresenta il cuore e il principale collo di bottiglia di MapReduce.

### 4.1 Shuffle

Durante lo shuffle:

* i dati intermedi vengono partizionati per chiave
* ogni reducer recupera dati da tutti i mapper
* avviene un intenso traffico di rete

### 4.2 Sort

Prima di essere consegnati ai reducer:

* i dati vengono ordinati per chiave
* i valori associati alla stessa chiave vengono raggruppati

Risultato logico:

```
(apple, [1,1])
(banana, [1])
(cherry, [1])
```

### 4.3 Impatto sulle prestazioni

Lo shuffle comporta:

* elevato I/O di rete
* uso intensivo del disco
* sensibilità allo skew delle chiavi

---

## 5. Fase REDUCE

Il Reducer riceve coppie del tipo:

```
(chiave, lista_di_valori)
```

Applica una funzione di aggregazione (somma, media, conteggio, ecc.) e produce l’output finale, che viene scritto sul filesystem distribuito.

Esempio:

```
Reducer input: (apple, [1,1])
Output: (apple, 2)
```

Ogni chiave è elaborata da un solo reducer, il che rende la fase deterministica ma vulnerabile a problemi di sbilanciamento del carico.

---

## 6. Combiner

Il Combiner è una riduzione locale opzionale eseguita dopo la fase Map.

Scopo:

* ridurre la quantità di dati trasferiti durante lo shuffle

Limitazioni:

* utilizzabile solo con funzioni associative e commutative
* non è garantito che venga eseguito

---

## 7. Fault tolerance

MapReduce è progettato per operare su cluster di nodi non affidabili.

Meccanismi principali:

* riavvio automatico dei task falliti
* utilizzo delle repliche dei dati
* speculative execution per task lenti

La persistenza su disco delle fasi intermedie consente il recupero senza ricalcolare l’intero job.

---

## 8. MapReduce e Hive

Hive traduce query SQL-like in uno o più job MapReduce (o Tez/Spark).

Esempio:

```
SELECT categoria, COUNT(*)
FROM vendite
GROUP BY categoria;
```

Viene trasformata in:

* Map: (categoria, 1)
* Shuffle: raggruppamento per categoria
* Reduce: somma dei valori

Query complesse con JOIN producono catene di job consecutivi, aumentando la latenza complessiva.

---

## 9. Limiti di MapReduce

Le principali cause di lentezza sono:

* overhead di avvio dei container
* uso intensivo del disco
* traffico di rete durante lo shuffle
* assenza di un’esecuzione a grafo (DAG)

MapReduce è quindi poco adatto a carichi interattivi o iterativi.

---

## 10. Evoluzione: Tez e Spark

Per superare i limiti di MapReduce sono nati motori più moderni:

* **Tez**: introduce l’esecuzione a DAG mantenendo il paradigma batch
* **Spark**: utilizza l’elaborazione in memoria e un DAG globale, riducendo drasticamente la latenza

Nei sistemi moderni, Hive utilizza di default Tez o Spark invece di MapReduce puro.

---

## 11. Conclusione

MapReduce rappresenta un modello di computazione distribuita che privilegia in modo esplicito **scalabilità, affidabilità e semplicità concettuale** rispetto alla latenza di esecuzione. Il suo funzionamento si basa su un paradigma funzionale stateless, in cui le funzioni Map e Reduce operano su grandi collezioni di coppie chiave–valore senza mantenere stato condiviso.

Dal punto di vista computazionale, MapReduce realizza un parallelismo massivo sui dati attraverso la suddivisione dell’input in split indipendenti, ma impone una struttura a fasi rigidamente sincronizzate. In particolare, la fase di Shuffle & Sort introduce una barriera globale che, pur consentendo operazioni parallele locali, limita il parallelismo temporale complessivo del job.

Queste scelte architetturali rendono MapReduce estremamente robusto in ambienti distribuiti e soggetti a guasti, ma al tempo stesso poco adatto a elaborazioni iterative o interattive. Motori più moderni come Tez e Spark nascono proprio per superare tali limiti, mantenendo però molti dei principi introdotti da MapReduce.

La comprensione approfondita di MapReduce resta quindi fondamentale non solo per lo studio dei sistemi Big Data tradizionali, ma anche per comprendere l’evoluzione degli attuali engine di elaborazione distribuita.

MapReduce rappresenta una pietra miliare nel mondo dei Big Data: semplice, robusto e scalabile. Tuttavia, la sua architettura batch e disk-based lo rende inadatto ai casi d’uso moderni a bassa latenza.

Comprendere MapReduce resta fondamentale per capire l’evoluzione degli engine distribuiti e le scelte architetturali alla base degli attuali sistemi di data processing.



## APACHE HIVE

**Cos'è**: Data Warehouse + SQL-on-Hadoop

**Cosa fa**
- Definisce e governa tabelle, schemi, partizioni
- Gestisce metadati via Hive Metastore
- Traduce query HiveQL in MapReduce, Tez o Spark
- Interroga HDFS, S3, ADLS

**Quando usarlo**
- ETL complessi con molte fasi: join, aggregazioni, dedupliche, spill su disco
- Analisi su grandi volumi di dati (terabyte+)
- Batch notturni programmati
- Quando serve scalabilità orizzontale e fault tolerance
- NON per interrogazioni real-time (latenza troppo alta)

---

## APACHE IMPALA

**Cos'è**: Motore SQL MPP in-memory (Quindi non ha la parte wharehouse)

**Cosa fa**
- Interroga gli stessi dati di Hive (su HDFS/S3)
- Usa gli stessi metadati (Hive Metastore)
- Fornisce SQL interattivo con bassa latenza

**Quando usarlo**
- BI, dashboard, report
- Analisi esplorativa e ad-hoc
- Query puntuali e veloci
- NON per ETL pesanti (limiti di memoria, nessuna fault tolerance)
- NON per batch notturni lunghi (architettura in-memoria, richiede query snelle)

---

## APACHE SQOOP

**Cos'è**: Strumento di data transfer (non motore SQL)

**Cosa fa**
- Importa dati da database relazionali (Oracle, MySQL, PostgreSQL, SQL Server, ecc.) verso Hadoop (HDFS/Hive)
- Esporta dati da Hadoop verso database relazionali
- Basato su MapReduce

**Quando usarlo**
- Ingestione iniziale da DB a Hadoop
- Estrazione da Hadoop verso DB
- NON per query o analisi

---

## TRANSAZIONI E ACID

**Database Relazionali Classici** (Oracle, MySQL, PostgreSQL)
- Architettura integrata: storage + motore SQL + gestore transazioni in un sistema unico
- UPDATE/DELETE puntuali: localizzazione veloce di singole righe via indici
- **ACID garantito**:
  - **A**tomicity: update intera o non accade
  - **C**onsistency: database rimane in stato valido
  - **I**solation: operazioni concorrenti non interferiscono
  - **D**urability: dati committed sono persistenti anche se crash
- Scalabilità: verticale (server potente)
- Caso d'uso: transazioni online (OLTP), e-commerce, CRM

**Hadoop/Hive/Impala**
- Architettura separata: storage (HDFS/S3) + motori SQL
- UPDATE/DELETE implicano riscrivere file interi (molto lento per volumi grandi)
- **ACID non nativo**: due scritture concorrenti = risultato imprevedibile; crash durante scrittura = dati parziali rimangono
- Scalabilità: orizzontale (molti nodi a basso costo)
- Caso d'uso: big data analytics (OLAP), ETL batch, machine learning
- Soluzione moderna: **Delta Lake**, **Iceberg**, **Hudi** aggiungono log transazionali e snapshot per ACID su Hadoop

**Quando scegliere**
- **Relazionali**: transazioni critiche, UPDATE/DELETE frequenti, latenza bassa, volume moderato (GB/TB)
  - Esempi: banca (prelievi atomici), e-commerce (ordini), CRM (contatti aggiornati)
- **Hadoop**: big data, elaborazioni batch, dati immutabili (insert-once), latenza accettabile, volume enorme (TB/PB)
  - Esempi: analytics, ML su terabyte di dati, log analysis, reporting notturni


# Perché non si usa ACID non nativo nelle transazioni bancarie?
Le transazioni bancarie richiedono la massima affidabilità: ogni operazione (es. bonifico, prelievo) deve essere completata interamente o non avvenire affatto, senza possibilità di errori o dati parziali.
Un sistema “ACID non nativo” (come Hadoop/Impala/Hive senza Delta/Iceberg/Hudi) non garantisce atomicità, isolamento e durabilità in modo rigoroso: in caso di crash, scritture concorrenti o errori, i dati potrebbero risultare incompleti o corrotti.
Per questo, in ambito bancario si usano solo database relazionali classici (Oracle, PostgreSQL, MySQL, ecc.) che implementano ACID in modo nativo e robusto.
Impala ha delle somiglianze con i database relazionali classici?
Sì, Impala offre un’interfaccia SQL molto simile (SELECT, JOIN, GROUP BY, ecc.) e permette di interrogare dati strutturati come un database relazionale.
Tuttavia, a differenza dei DB classici:
Non gestisce transazioni ACID su singole righe (no UPDATE/DELETE atomici)
Non ha indici o constraint come PK/FK
I dati sono su file distribuiti (HDFS/S3), non su storage integrato
Ottimizzato per analisi su grandi volumi, non per OLTP
In sintesi: Impala “sembra” un database relazionale per chi scrive SQL, ma sotto è progettato per analisi big data, non per transazioni critiche. Vuoi aggiungere queste spiegazioni al documento?

---

## DISEGNO LOGICO A STRATI

```
┌─────────────────────────┐
│   UTENTI / BI           │
│ (report, dashboard, SQL)│
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│    MOTORI SQL           │
│ (Impala, Hive, Spark)   │
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│     METADATI            │
│   (Hive Metastore)      │
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│      STORAGE            │
│  (HDFS / S3 / ADLS)     │
│  ▶ QUI STANNO LE RIGHE  │
└─────────────────────────┘
```

---

## CASO REALE: AMAZON (ARCHITETTURA IBRIDA)

Amazon usa entrambi i sistemi:

**MySQL/Aurora** (DB relazionale - OLTP)
- Carrello acquisti: UPDATE immediato (ACID)
- Ordini: INSERT + UPDATE stock (transazione atomica)
- Pagamenti: atomicità critica (tutto o nulla)
- Inventario real-time: decremento immediato
- Latenza: millisecondi

**Hadoop/EMR/S3** (Data lake/warehouse - OLAP)
- Raccomandazioni: miliardi di transazioni storiche analizzate
- Analytics e BI: report su vendite, trend, previsioni
- Click-stream analysis: miliardi di click e navigazioni
- Machine learning: terabyte di dati per addestrare modelli
- Data science: A/B test, segmentazione, pricing
- Latenza: secondi/minuti/ore

**Flusso**:
1. Cliente ordina → **MySQL** registra (real-time, ACID)
2. Di notte → **Sqoop** esporta ordini a **S3** (Ossia datastore distribuito)
3. **EMR**(servizio che gestisce cluster hadoop) elabora milioni di ordini
4. Risultati in **Redshift** (DW) o **S3** (data lake)
5. Analytics, ML e BI leggono da Redshift/S3

**Sintesi**: MySQL = sistema operativo (transazioni live); Hadoop = sistema analitico (big data, ML, BI su dati storici).

---

## RIEPILOGO FINALE

| Componente    | Ruolo                                | Possiede/Governa Dati | Latenza           |
|---------------|--------------------------------------|----------------------|------------------- |
| **Storage**   | Memorizza file fisici                | Possiede              | N/A               |
| **Metastore** | Catalogo e governance (tecnica?)     | Governa metadati      | N/A               |
| **Hive**      | DW + query batch                     | Governa (non possiede) | Secondi/minuti   |
| **Impala**    | Query interattivo MPP in-memory      | Governa (non possiede) | Secondi/milli    |
| **Sqoop**     | Data transfer DB ↔ Hadoop            | Trasporta (non governa) | Batch           |

**Governance vs Possesso**:
- Lo **storage possiede** fisicamente i dati (file su HDFS/S3/ADLS)
- Il **data warehouse governa** i dati (metadati, schemi, tabelle, sicurezza, processi)
- I **motori SQL leggono/scrivono** i dati secondo il governo del warehouse

**Nota importante:**
- **Data Warehouse** (Hive, Snowflake) = esegue query SQL e governa tecnicamente
- **Data Catalog** (Alation, Collibra) = documenta significato business (NON esegue query)

# Cloudera runtime

Cloudera Runtime è l'insieme completo degli strumenti open source per storage distribuito, elaborazione batch/streaming, SQL, NoSQL, ML, ingestion e orchestrazione dati.


Cloudera Runtime comprende:

Storage:

HDFS (Hadoop Distributed File System)
Apache Ozone (object store)
Elaborazione Dati:

Apache Hadoop MapReduce (batch processing)
Apache Spark (batch + streaming + ML)
Apache Hive (SQL data warehouse)
Apache Impala (interactive SQL)
Apache Pig (data flow scripting)
Streaming e Real-time:

Apache Kafka (messaging/streaming)
Apache Flink (stream processing)
Spark Streaming
Apache NiFi (data flow automation)
Database NoSQL:

Apache HBase (wide-column store)
Apache Kudu (columnar storage engine)
Resource Management:

Apache YARN (cluster resource manager)
Data Ingestion:

Apache Sqoop (DB ↔ Hadoop transfer)
Apache Flume (log aggregation)
Search e Indexing:

Apache Solr (full-text search)
Coordinamento:

Apache ZooKeeper (distributed coordination)
Workflow:

Apache Oozie (job scheduling/orchestration)
Metadata:

Hive Metastore (catalog centrale)
Machine Learning:

Spark MLlib
In sintesi: Cloudera Runtime è l'insieme completo degli strumenti open source per storage distribuito, elaborazione batch/streaming, SQL, NoSQL, ML, ingestion e orchestrazione dati.


Cloudera Runtime può essere concettualmente paragonato a vSphere nel mondo VMware, in quanto rappresenta il livello di runtime che abilita l’esecuzione dei carichi di lavoro. Tuttavia, mentre vSphere opera a livello infrastrutturale come piattaforma di virtualizzazione, Cloudera Runtime opera a livello applicativo come runtime per workload Big Data e analytics, collocandosi sopra lo strato di virtualizzazione o cloud.

# VMware stack
Hardware
↓
vSphere
↓
VM / OS
↓
Applicazioni

# Cloudera stack
Hardware / Cloud
↓
OS / Container / VM
↓
Cloudera Runtime
↓
Spark / Hive / Hadoop / ML
↓
Data workload

| **Dimensione**               | **Cloudera Runtime (CDP)**                             | **vSphere (VMware)**                          |
| ---------------------------- | ------------------------------------------------------ | ----------------------------------------------|
| **Dominio**                  | Data Platform / Big Data / Analytics                   | Virtualizzazione infrastrutturale             |
| **Livello dello stack**      | Applicativo–dati                                       | Infrastrutturale                              |
| **Funzione principale**      | Eseguire workload dati distribuiti                     | Eseguire macchine virtuali                    |
| **Tipo di workload**         | Spark, Hive, Hadoop, Impala, ML                        | VM generiche (applicazioni, database, servizi)|
| **Astrazione fornita**       | Cluster dati e motori di elaborazione                  | CPU, memoria, storage, rete                   |
| **Gestione risorse**         | Scheduling e resource management (es. YARN)            | Scheduling e resource management (DRS)        |
| **Unità di esecuzione**      | Job, query, applicazioni distribuite                   | Macchine virtuali                             | 
| **Dipendenza dall’OS**       | Richiede un OS sottostante (bare metal, VM, container) | Include un hypervisor che sostituisce l’OS host|
| **Rapporto con l’hardware**  | Indiretto                                              | Diretto                                       |
| **Collocazione tipica**      | Sopra VM / container / cloud                           | Direttamente sopra l’hardware                 |
| **Ruolo nella piattaforma**  | Runtime dei dati                                       | Runtime dell’infrastruttura                   |
| **Ambiente di riferimento**  | CDP (Public Cloud / Data Center)                       | Data center virtualizzato                     |
| **Esempi di componenti**     | Hadoop, Spark, Hive, Impala                            | ESXi, vMotion, HA, DRS                        |
| **Obiettivo architetturale** | Standardizzare l’elaborazione dei dati                 | Standardizzare l’uso dell’hardware            |
| **Governance e sicurezza**   | Integrata tramite SDX (Ranger, Atlas)                  | Demandata a strumenti esterni o superiori     |
| **Relazione reciproca**      | Può girare sopra vSphere                               | Può ospitare Cloudera Runtime                 |

## CDP Public Cloud vs CDP Private Cloud

| **Dimensione**                | **CDP Public Cloud**                                    | **CDP Private Cloud**                                  |
| ----------------------------- | ------------------------------------------------------- | ------------------------------------------------------ |
| **Infrastruttura**            | AWS, Azure, Google Cloud (gestita dal provider)         | On-premise o cloud privato (gestita dall'azienda)      |
| **Deployment**                | SaaS-like, provisioning automatico                      | Installazione manuale su cluster dedicati             |
| **Gestione cluster**          | Automatica (Cloudera gestisce upgrade e patching)       | Manuale (IT interno gestisce tutto)                    |
| **Scalabilità**               | Elastica, scale up/down on-demand                       | Limitata dalla capacità fisica del data center         |
| **Costi**                     | Pay-as-you-go (consumo effettivo)                       | CapEx: hardware + licenze + manutenzione               |
| **Time-to-value**             | Rapido (minuti/ore)                                     | Lento (settimane/mesi per setup)                       |
| **Manutenzione**              | Cloudera gestisce infrastruttura e runtime              | IT interno gestisce hardware, OS, runtime              |
| **Sicurezza dati**            | Multi-tenant, dati su cloud pubblico (cifratura)        | Single-tenant, dati rimangono on-premise               |
| **Compliance**                | Conforme a standard cloud (SOC2, ISO, GDPR)             | Controllo totale per requisiti specifici (HIPAA, PCI)  |
| **Networking**                | VPC, connessioni cloud-native                           | Rete aziendale interna                                 |
| **Disaster Recovery**         | Nativo cloud (multi-region, backup automatici)          | Richiede setup dedicato (backup, replica)              |
| **Flessibilità hardware**     | Provider cloud decide (preset configurazioni)           | Controllo totale su hardware e configurazione          |
| **Workload ideali**           | Analytics, ML, BI con carichi variabili                 | Workload critici, dati sensibili, compliance rigoroso  |
| **Esempi casi d'uso**         | Startup, progetti sperimentali, burst capacity          | Banche, sanità, governativi, legacy integration        |
| **Dipendenza vendor**         | Forte (Cloudera + cloud provider)                       | Moderata (Cloudera software, hardware proprio)         |
| **Aggiornamenti**             | Automatici, gestiti da Cloudera                         | Pianificati e applicati dall'IT interno                |
| **Costo prevedibilità**       | Variabile (dipende dall'uso)                            | Fisso (hardware ammortizzato)                          |
| **Skills richiesti**          | Cloud-native, meno sysadmin                             | Sysadmin, networking, storage management               |

**Scelta strategica:**
- **Public Cloud**: velocità, elasticità, riduzione complessità operativa
- **Private Cloud**: controllo, sicurezza, compliance, integrazione legacy


CDP Public Cloud pone l’accento sull’utilizzo dell’object store del provider cloud, invece di HDFS come avveniva in CDH e HDP. Questo determina una separazione tra calcolo e storage, consentendo a ciascun workload di disporre della propria capacità di elaborazione pur continuando ad accedere agli stessi dati sottostanti.

Poiché le implementazioni di CDH e HDP collocano insieme storage e calcolo, esse non sono adatte a workload transitori. In CDP Public Cloud (così come in CDP Private Cloud), gli amministratori possono registrare tutti gli ambienti di cui hanno bisogno.