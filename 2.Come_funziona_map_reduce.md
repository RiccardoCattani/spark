# COME FUNZIONA MAPREDUCE

## 1. Introduzione
MapReduce è un framework di programmazione distribuita introdotto per risolvere in modo sistematico il problema dell’elaborazione di dataset di dimensioni tali da non poter essere gestiti da un singolo nodo. Il suo obiettivo principale non è la velocità di risposta, bensì la **scalabilità orizzontale**, la **robustezza ai guasti** e la **semplicità del modello di programmazione**.

Storicamente, MapReduce nasce per affrontare contesti caratterizzati da:
- grandi volumi di dati (terabyte o petabyte)
- utilizzo di hardware non affidabile (commodity hardware)
- necessità di elaborazioni batch ripetibili e deterministiche

Il paradigma accetta deliberatamente una maggiore latenza in cambio della capacità di:
- suddividere automaticamente il lavoro
- recuperare da errori di nodo o di processo
- garantire risultati consistenti anche in presenza di failure parziali

MapReduce non va quindi interpretato come un semplice modello astratto, ma come un’**architettura di esecuzione completa**, profondamente integrata con il filesystem distribuito e il resource manager del cluster.

### 1.1 Principi di progettazione
Alla base di MapReduce vi sono alcuni principi architetturali fondamentali:
- **Move computation to data**: il codice viene eseguito dove risiedono i dati
- **Materializzazione su disco**: ogni fase critica produce output persistente
- **Stateless computation**: i task non mantengono stato condiviso
- **Determinismo**: a parità di input, l’output è sempre lo stesso

Questi principi spiegano sia i punti di forza sia i limiti del modello. Il paradigma si fonda sulla scomposizione del problema in tre fasi logiche principali:
- **Map**, che trasforma e proietta i dati
- **Shuffle & Sort**, che raggruppa i dati per chiave
- **Reduce**, che aggrega i risultati intermedi

## 2. Architettura generale
MapReduce opera sopra un filesystem distribuito (come HDFS) e un resource manager (come YARN). Il flusso logico dell’elaborazione è il seguente:
- i dati di input sono memorizzati in file distribuiti e suddivisi in blocchi
- ogni blocco è elaborato da un task di tipo **Map**
- i risultati intermedi vengono ordinati e ridistribuiti tra i nodi
- i task **Reduce** producono l’output finale

L’intera esecuzione è coordinata dal framework, che gestisce pianificazione, località dei dati e riavvio dei task in caso di errore.

## 3. Parallelizzazione in MapReduce
### 3.1 Nodi e mapper (chiarimento)
È essenziale distinguere tra **nodo** e **mapper**:
- un **nodo** è una macchina fisica o virtuale del cluster
- un **mapper** è un task di calcolo, cioè un processo eseguito su un nodo

I mapper non coincidono con i nodi: uno stesso nodo può eseguire più mapper in parallelo. Ogni mapper è associato a uno **split** di input, cioè una porzione logica dei dati assegnata a un singolo task.

Il grado di parallelismo non dipende direttamente dal numero di nodi, ma dal numero di task Map eseguibili contemporaneamente, che dipende dal numero di split e dalle risorse disponibili.

### 3.2 Parallelismo sui dati
Il parallelismo principale deriva dalla suddivisione dei dati di input in split indipendenti. Ogni split può essere elaborato da un mapper distinto.

Se un file è suddiviso in $N$ split, il framework può avviare fino a $N$ mapper in parallelo, compatibilmente con le risorse del cluster.

È importante notare che:
- uno split logico non coincide necessariamente con un blocco HDFS
- uno stesso nodo può eseguire più mapper contemporaneamente

Questo modello consente una scalabilità quasi lineare aumentando il numero di nodi.

### 3.3 Parallelismo dei mapper
Ogni mapper:
- opera in modo indipendente
- non condivide stato con altri mapper
- può essere riavviato senza effetti collaterali

Questa indipendenza rende il modello altamente parallelo e tollerante ai guasti.

### 3.4 Parallelismo nella fase Reduce
Anche la fase Reduce è parallelizzabile. Il numero di reducer è configurabile e determina il grado di parallelismo dell’aggregazione.

Ogni reducer:
- riceve un sottoinsieme disgiunto delle chiavi
- elabora le chiavi in modo sequenziale

Il parallelismo complessivo della fase Reduce è quindi limitato dal numero di reducer configurati.

### 3.5 Parallelizzazione della fase Shuffle & Sort
La fase di Shuffle & Sort è distribuita e parzialmente parallelizzabile.

**Lato Mapper (map-side shuffle)**
Ogni mapper, in modo indipendente:
- partiziona il proprio output in base ai reducer
- ordina le coppie chiave–valore
- scrive i dati su disco locale

**Lato Reducer (reduce-side shuffle)**
Ogni reducer, in parallelo agli altri:
- contatta tutti i mapper
- scarica solo la propria partizione di dati
- esegue operazioni locali di merge e sort

Il trasferimento dei dati segue uno schema **many-to-many**, con elevato traffico di rete.

### 3.6 Limiti della parallelizzazione
Nonostante l’elevato parallelismo locale, MapReduce presenta limiti strutturali:
- separazione rigida tra fasi Map e Reduce
- barriera globale di sincronizzazione durante lo shuffle
- problemi di sbilanciamento delle chiavi (data skew)
- impossibilità di avviare i reducer prima del completamento dei mapper

Il modello è quindi massivamente parallelo, ma sincronizzato a fasi.

## 4. Fase Map
### 4.1 Input e split
I file di input vengono suddivisi in split logici, generalmente allineati ai blocchi del filesystem distribuito. Ogni split è assegnato a un mapper.

### 4.2 Funzionamento del mapper
Il mapper:
- legge i record tramite un **RecordReader**
- applica una funzione definita dall’utente
- produce coppie chiave–valore intermedie

**Esempio (conteggio parole)**
```
Input:  apple banana apple cherry
Output: (apple, 1)
	(banana, 1)
	(apple, 1)
	(cherry, 1)
```

### 4.3 Scrittura su disco locale
Le coppie intermedie vengono:
- mantenute in un buffer in memoria
- ordinate per chiave
- scritte su disco locale (spill)

Questa scelta aumenta la tolleranza ai guasti, ma introduce latenza.

## 5. Fase Shuffle & Sort
Durante lo shuffle:
- i dati vengono partizionati per chiave
- ogni reducer recupera dati da tutti i mapper
- si genera un intenso traffico di rete

Prima della fase Reduce, i dati vengono ordinati e raggruppati per chiave:
```
(apple, [1, 1])
(banana, [1])
(cherry, [1])
```
Lo shuffle rappresenta il principale collo di bottiglia del modello.

## 6. Fase Reduce
Il reducer riceve coppie del tipo:
```
(chiave, lista_di_valori)
```
Applica una funzione di aggregazione e produce l’output finale sul filesystem distribuito. Ogni chiave è elaborata da un solo reducer, garantendo determinismo ma esponendo a possibili problemi di sbilanciamento.

Dopo la fase Map e Shuffle & Sort, il reducer riceve:
```
(apple, [1, 1])
(banana, [1])
(cherry, [1])
```
Il risultato finale della fase Reduce sarà:
```
(apple, 2)
(banana, 1)
(cherry, 1)
```
Quindi, Reduce ha aggregato i valori associati a ciascuna chiave (parola), restituendo il conteggio totale per ogni parola.

## 7. Combiner
Il **combiner** è una riduzione locale opzionale eseguita dopo la fase Map, con lo scopo di ridurre la quantità di dati trasferiti nello shuffle.

È utilizzabile solo con funzioni associative e commutative e non è garantito che venga eseguito.

## 8. Fault tolerance
MapReduce è progettato per operare su cluster non affidabili. I principali meccanismi includono:
- riavvio automatico dei task falliti
- utilizzo delle repliche dei dati
- speculative execution per task lenti

La persistenza su disco delle fasi intermedie consente il recupero senza ricalcolare l’intero job.

## 9. MapReduce e Hive
Hive traduce query SQL-like in uno o più job MapReduce (o Tez/Spark).

**Esempio**
```
SELECT categoria, COUNT(*)
FROM vendite
GROUP BY categoria;
```
Questa query viene trasformata in fasi Map, Shuffle e Reduce. Query complesse con JOIN producono catene di job consecutivi, aumentando la latenza.

## 10. Limiti ed evoluzione
Le principali cause di lentezza di MapReduce sono:
- overhead di avvio dei task
- uso intensivo del disco
- traffico di rete nello shuffle
- assenza di un DAG globale

Per superare questi limiti sono nati:
- **Tez**, che introduce un’esecuzione a DAG
- **Spark**, che sfrutta l’elaborazione in memoria

Nei sistemi moderni, Hive utilizza di default Tez o Spark.

## 11. Quando usare MapReduce
**Adatto a**
- elaborazioni batch su grandi volumi
- job deterministici con input immutabile
- workload che tollerano latenza elevata

**Meno adatto a**
- carichi iterativi o interattivi
- pipeline con molte dipendenze tra step
- casi con forte sbilanciamento delle chiavi

## 12. Conclusione
MapReduce privilegia scalabilità, affidabilità e semplicità concettuale rispetto alla latenza. Realizza un parallelismo massivo sui dati, ma impone una struttura a fasi rigidamente sincronizzate.

Queste scelte lo rendono estremamente robusto in ambienti distribuiti, ma poco adatto a carichi iterativi o interattivi. Comprendere MapReduce resta comunque fondamentale per capire l’evoluzione degli attuali motori di elaborazione distribuita e le scelte architetturali alla base dei sistemi Big Data moderni.