

## APACHE HIVE

**Cos'è**: Data Warehouse + SQL-on-Hadoop

**Cosa fa**
- Definisce e governa tabelle, schemi, partizioni
- Gestisce metadati via Hive Metastore
- Traduce query HiveQL in MapReduce, Tez o Spark
- Interroga HDFS, S3, ADLS

**Quando usarlo**
- ETL complessi con molte fasi: join, aggregazioni, dedupliche, spill su disco*
- Analisi su grandi volumi di dati (terabyte+)
- Batch notturni programmati
- Quando serve scalabilità orizzontale e fault tolerance
- NON per interrogazioni real-time (latenza troppo alta)

* 
ETL Complessi con molte fasi - Spiegazione dettagliata
ETL = Extract, Transform, Load

1. JOIN (Unire tabelle)
Combina dati da tabelle diverse basandosi su chiavi comuni.

Esempio pratico:

-- Tabella Ordini (10 milioni di righe)
SELECT * FROM ordini
JOIN clienti ON ordini.cliente_id = clienti.id
JOIN prodotti ON ordini.prodotto_id = prodotti.id

Perché serve Hive:

- Con terabyte di dati, un join richiede shuffle dei dati tra i nodi del cluster*
- MapReduce/Tez distribuiscono l'elaborazione su decine/centinaia di macchine
- Impala fallirebbe per mancanza di memoria

*
Come funziona lo Shuffle nei JOIN tra nodi
Lo shuffle è il processo di ridistribuzione dei dati tra i nodi del cluster per permettere il join. Ti faccio un esempio pratico:

Scenario: Join di due tabelle
Supponiamo di voler unire:

- Tabella CLIENTI (1 TB, 100 milioni righe)
- Tabella ORDINI (5 TB, 1 miliardo righe)

SELECT o.ordine_id, c.nome, o.importo
FROM ordini o
JOIN clienti c ON o.cliente_id = c.id

Il problema senza shuffle

HDFS ha i dati così distribuiti:

Nodo 1: ordini (cliente_id=1,3,7,9,...)
        clienti (cliente_id=2,5,8,...)
        ❌ Nodo 1 non ha TUTTI i clienti di cui ha ordini!

Nodo 2: ordini (cliente_id=2,4,6,10,...)
        clienti (cliente_id=1,3,9,...)
        ❌ Mismatch completo!

Senza shuffle: Nodo 1 farebbe join tra ordini locali e clienti locali → RISULTATO INCOMPLETO/SBAGLIATO ❌

La soluzione: Shuffle
FASE 1: Map (Lettura e pre-processing)

Nodo 1 legge:
  Ordine 100: cliente_id=5 → Emetto (5, "ordine_100")
  Ordine 101: cliente_id=3 → Emetto (3, "ordine_101")
  Ordine 102: cliente_id=5 → Emetto (5, "ordine_102")

Nodo 2 legge:
  Ordine 200: cliente_id=5 → Emetto (5, "ordine_200")
  Ordine 201: cliente_id=3 → Emetto (3, "ordine_201")

Nodo 3 legge (tabella clienti):
  Cliente 3: ... → Emetto (3, "cliente_3")
  Cliente 5: ... → Emetto (5, "cliente_5")

FASE 2: Shuffle & Sort (RIDISTRIBUZIONE in RETE)

Tutti gli oggetti con la STESSA chiave vengono mandati allo STESSO nodo.

Esempio: Tutti i record con cliente_id=5 vanno a Nodo X

┌──────────────┐
│   Rete       │  ← I dati volano tra nodi!
│  (shuffle)   │
└──────────────┘

Risultato dopo shuffle:

Nodo A:
  (3, "ordine_101")
  (3, "ordine_201")
  (3, "cliente_3")     ← TUTTI i record con chiave 3

Nodo B:
  (5, "ordine_100")
  (5, "ordine_102")
  (5, "ordine_200")
  (5, "cliente_5")     ← TUTTI i record con chiave 5

FASE 3: Reduce (Join locale)

Nodo A (ha chiave 3):
  (3, "ordine_101") JOIN (3, "cliente_3")  → Risultato match
  (3, "ordine_201") JOIN (3, "cliente_3")  → Risultato match

Nodo B (ha chiave 5):
  (5, "ordine_100") JOIN (5, "cliente_5")  → Risultato match
  (5, "ordine_102") JOIN (5, "cliente_5")  → Risultato match
  (5, "ordine_200") JOIN (5, "cliente_5")  → Risultato match

Visualizzazione del flusso

MAP PHASE (Lettura)
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  Nodo 1     │  │  Nodo 2     │  │  Nodo 3     │
│  Ordini     │  │  Ordini     │  │  Clienti    │
└────────┬────┘  └────────┬────┘  └────────┬────┘
         │                │                │
         │ Emetto chiave  │                │
         │ (cliente_id)   │                │
         └────────┬───────┴────────┬───────┘
                  │                │
    SHUFFLE PHASE (RIDISTRIBUZIONE IN RETE)
    ┌────────────────────────────────────┐
    │  "Raggruppa per cliente_id"         │
    │  Sposta i dati sulla rete (I/O)     │
    └──────────────┬─────────────────────┘
                   │
    REDUCE PHASE (Join locale)
    ┌──────────────┴─────────────┐
    │                             │
┌───────────────┐         ┌──────────────┐
│  Nodo X       │         │  Nodo Y      │
│ cliente_id=3  │         │ cliente_id=5 │
│  Ordini +     │         │  Ordini +    │
│  Clienti      │         │  Clienti     │
└───────────────┘         └──────────────┘
    (JOIN OK)                  (JOIN OK)

Perché lo shuffle è lento?
a) I/O di rete è il bottleneck
- Scrivere dati su disco (HDFS) dal Map
- Trasferire dati sulla rete (molto lento vs RAM)
- Leggere dati dal disco nel Reduce

b) Ordine: Per ogni gruppo di chiavi uguali, i dati vengono ordinati
- Necessario per unire ordini e clienti con la stessa chiave
- Ordinamento = CPU + I/O costosi

c) Tempo totale: 1 TB di dati potrebbe impiegare minuti-ore per lo shuffle

Perché serve il Shuffle
Senza shuffle: Non potresti comparare record da diversi nodi → join sarebbe incompleto

Con shuffle: Garantisce che TUTTI i record con la stessa chiave siano nello stesso nodo → join completo e corretto

Differenza: Hive vs Impala
Aspetto	Hive	Impala
Shuffle	✓ Crea file temporanei su disco durante shuffle	✗ Tenta di fare tutto in memoria
Spill	✓ Se memoria insufficiente, usa disco	✗ Crash se out-of-memory
Join su 1TB	Lento ma completa (shuffle + spill)	Veloce se c'è RAM, fallisce se no
Affidabilità	Alta (recupera da disco se nodo muore)	Bassa (deve ripartire tutto)

Scenario: Join di 500 GB
Memoria disponibile: 128 GB

Durante lo SHUFFLE di Hive:

Fase 1: Shuffle 100 GB per cliente_id=1,2,3
        ✓ Carica in memoria (128 GB disponibili)

Fase 2: Comincia shuffle per cliente_id=4,5,6
        ✗ Memoria piena!
        ✓ SPILL: scrive risultati di fase 1 su disco HDFS
        ✓ Libera memoria
        ✓ Carica fase 2

Impala stessa situazione:
        ✗ Memoria piena
        ✗ Crash: query fails
        ✗ Deve ripartire da capo

2. AGGREGAZIONI (Calcoli su gruppi)
Operazioni come SUM, COUNT, AVG, MAX su grandi dataset.

Esempio:
-- Totale vendite per regione e prodotto (miliardi di transazioni)
SELECT regione, prodotto, SUM(importo), COUNT(*), AVG(sconto)
FROM vendite
GROUP BY regione, prodotto

Perché serve Hive:

- Aggregare miliardi di righe richiede memoria enorme
- Hive può utilizzare dischi temporanei per operazioni intermedie

3. DEDUPLICHE (Rimuovere duplicati)
Identificare e eliminare record ripetuti.

Esempio:
-- Rimuovere duplicati da 500 milioni di righe
SELECT DISTINCT cliente_id, email, telefono
FROM contatti_raw

Perché serve Hive:

Confrontare ogni riga con tutte le altre richiede sorting/hashing massiccio
Processo computazionalmente intensivo su grandi volumi

4. SPILL SU DISCO (Il vantaggio chiave di Hive)
Cos'è:
Quando l'operazione richiede più memoria di quella disponibile, Hive scrive risultati intermedi su disco invece di fallire.

Scenario tipico:
1. Join di 2 tabelle da 1TB ciascuna
2. La memoria RAM disponibile è 128GB
3. Hive:
   ✓ Processa una parte in memoria
   ✓ Scrive risultati intermedi su disco (HDFS)
   ✓ Libera la memoria
   ✓ Carica la parte successiva
   ✓ Continua fino a completamento

Impala:
   ✗ Tenta di caricare tutto in memoria
   ✗ Memoria esaurita → query fallita

Fault Tolerance:

Se un nodo si guasta durante l'elaborazione, Hive riesegue solo quella parte (grazie a MapReduce/Tez)
Impala deve ripartire da zero

5. PIPELINE ETL COMPLETA - Esempio Reale
Scenario: E-commerce che processa dati giornalieri

-- FASE 1: Extract - Carica dati grezzi (100 milioni di righe/giorno)
INSERT INTO staging_ordini
SELECT * FROM raw_ordini WHERE data = '2026-02-04';

-- FASE 2: Transform - Pulizia e deduplicazione
INSERT INTO ordini_puliti
SELECT DISTINCT 
  ordine_id,
  cliente_id,
  UPPER(TRIM(email)),  -- Normalizzazione
  CAST(importo AS DECIMAL(10,2))
FROM staging_ordini
WHERE importo > 0 AND cliente_id IS NOT NULL;

-- FASE 3: Join con dimensioni
INSERT INTO ordini_arricchiti
SELECT 
  o.ordine_id,
  o.importo,
  c.regione,
  c.segmento,
  p.categoria,
  p.fornitore
FROM ordini_puliti o
JOIN clienti c ON o.cliente_id = c.id
JOIN prodotti p ON o.prodotto_id = p.id;

-- FASE 4: Aggregazioni business
INSERT INTO report_giornaliero
SELECT 
  data,
  regione,
  categoria,
  SUM(importo) as totale_vendite,
  COUNT(DISTINCT cliente_id) as clienti_unici,
  AVG(importo) as scontrino_medio
FROM ordini_arricchiti
GROUP BY data, regione, categoria;

Perché Hive e non Impala:

Durata: 2-3 ore di elaborazione batch notturna (Impala non è progettato per job così lunghi)
Memoria: Join su centinaia di GB richiedono spill su disco
Affidabilità: Se un nodo si guasta alle 3:00 AM, Hive recupera automaticamente
Costo: Può usare nodi economici con meno RAM


Confronto visivo
Operazione	Dataset	Hive	Impala
Join 2 tabelle	10GB ciascuna	✓ Lento ma completa	✓ Veloce
Join 2 tabelle	500GB ciascuna	✓ Spill su disco, completa in ore	✗ Out of memory
GROUP BY	1 miliardo righe	✓ Con spill	✗ Rischio crash
Deduplica	10 milioni righe	✓	✓
Deduplica	5 miliardi righe	✓	✗
Job 8 ore	Batch notturno	✓ Progettato per questo	✗ Non
---

## APACHE IMPALA

**Cos'è**: Motore SQL MPP in-memory (Quindi non ha la parte wharehouse)

**Cosa fa**
- Interroga gli stessi dati di Hive (su HDFS/S3)
- Usa gli stessi metadati (Hive Metastore)
- Fornisce SQL interattivo con bassa latenza

**Quando usarlo**
- BI, dashboard, report
- Analisi esplorativa e ad-hoc
- Query puntuali e veloci
- NON per ETL pesanti (limiti di memoria, nessuna fault tolerance)
- NON per batch notturni lunghi (architettura in-memoria, richiede query snelle)

---

## APACHE SQOOP

**Cos'è**: Strumento di data transfer (non motore SQL)

**Cosa fa**
- Importa dati da database relazionali (Oracle, MySQL, PostgreSQL, SQL Server, ecc.) verso Hadoop (HDFS/Hive)
- Esporta dati da Hadoop verso database relazionali
- Basato su MapReduce

**Quando usarlo**
- Ingestione iniziale da DB a Hadoop
- Estrazione da Hadoop verso DB
- NON per query o analisi

---

## TRANSAZIONI E ACID

**Database Relazionali Classici** (Oracle, MySQL, PostgreSQL)
- Architettura integrata: storage + motore SQL + gestore transazioni in un sistema unico
- UPDATE/DELETE puntuali: localizzazione veloce di singole righe via indici
- **ACID garantito**:
a) Atomicity (Atomicità): Ogni transazione è indivisibile: o tutte le operazioni vengono completate, oppure nessuna. Se qualcosa va storto, il database annulla tutto, evitando dati parziali o incoerenti. Esempio: in un bonifico, il denaro viene scalato da un conto e accreditato sull’altro, oppure nessuna delle due operazioni avviene.

b) Consistency (Consistenza):
La consistenza assicura che ogni transazione trasformi il database da uno stato valido a un altro, rispettando tutte le regole definite (vincoli di integrità, chiavi primarie/esterne, trigger, tipi di dati, ecc.).
Esempio: se esiste un vincolo che impedisce saldi negativi, una transazione che porterebbe un conto sotto zero viene bloccata e annullata. Così il database non può mai contenere dati “impossibili” o incoerenti.

c) Isolation (Isolamento):
L’isolamento garantisce che le transazioni eseguite contemporaneamente non si disturbino a vicenda. Ogni transazione lavora come se fosse l’unica attiva: le modifiche di una non sono visibili alle altre finché non è completata (commit).
Questo evita problemi come:
- Letture sporche: leggere dati modificati da una transazione non ancora confermata.
- Letture non ripetibili: lo stesso dato letto due volte nella stessa transazione cambia perché un’altra transazione lo ha modificato nel     frattempo.
- Scritture perse: due transazioni aggiornano lo stesso dato e una sovrascrive l’altra.

L’isolamento è fondamentale per mantenere la correttezza dei dati in ambienti multiutente.

d)  Durability (Durabilità): Una volta che una transazione è confermata (commit), i dati sono persistenti: anche in caso di crash, blackout o guasti, le modifiche non vanno perse. Il database garantisce la sopravvivenza dei dati tramite log e meccanismi di recovery.
Questi principi sono:
- Scalabilità: verticale (server potente)
- Caso d'uso: transazioni online (OLTP), e-commerce, CRM

**Hadoop/Hive/Impala**
- Architettura separata: storage (HDFS/S3) + motori SQL
- UPDATE/DELETE implicano riscrivere file interi (molto lento per volumi grandi)
- **ACID non nativo**: due scritture concorrenti = risultato imprevedibile; crash durante scrittura = dati parziali rimangono
- Scalabilità: orizzontale (molti nodi a basso costo)
- Caso d'uso: big data analytics (OLAP), ETL batch, machine learning
- Soluzione moderna: **Delta Lake**, **Iceberg**, **Hudi** aggiungono log transazionali e snapshot per ACID su Hadoop

**Quando scegliere**
- **Relazionali**: transazioni critiche, UPDATE/DELETE frequenti, latenza bassa, volume moderato (GB/TB)
  - Esempi: banca (prelievi atomici), e-commerce (ordini), CRM (contatti aggiornati)
- **Hadoop**: big data, elaborazioni batch, dati immutabili (insert-once), latenza accettabile, volume enorme (TB/PB)
  - Esempi: analytics, ML su terabyte di dati, log analysis, reporting notturni

# Perché non si usa ACID non nativo nelle transazioni bancarie?
Le transazioni bancarie richiedono la massima affidabilità: ogni operazione (es. bonifico, prelievo) deve essere completata interamente o non avvenire affatto, senza possibilità di errori o dati parziali.
Un sistema “ACID non nativo” (come Hadoop/Impala/Hive senza Delta/Iceberg/Hudi) non garantisce atomicità, isolamento e durabilità in modo rigoroso: in caso di crash, scritture concorrenti o errori, i dati potrebbero risultare incompleti o corrotti.
Per questo, in ambito bancario si usano solo database relazionali classici (Oracle, PostgreSQL, MySQL, ecc.) che implementano ACID in modo nativo e robusto.
Impala ha delle somiglianze con i database relazionali classici?
Sì, Impala offre un’interfaccia SQL molto simile (SELECT, JOIN, GROUP BY, ecc.) e permette di interrogare dati strutturati come un database relazionale.
Tuttavia, a differenza dei DB classici:
- Non gestisce transazioni ACID su singole righe (no UPDATE/DELETE atomici)
- Non ha indici o constraint come PK/FK
- I dati sono su file distribuiti (HDFS/S3), non su storage integrato
- Ottimizzato per analisi su grandi volumi, non per OLTP
In sintesi: Impala “sembra” un database relazionale per chi scrive SQL, ma sotto è progettato per analisi big data, non per transazioni critiche. Vuoi aggiungere queste spiegazioni al documento?

---

## DISEGNO LOGICO A STRATI

```
┌─────────────────────────┐
│   UTENTI / BI           │
│ (report, dashboard, SQL)│
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│    MOTORI SQL           │
│ (Impala, Hive, Spark)   │
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│     METADATI            │
│   (Hive Metastore)      │
└────────────┬────────────┘
             │
             ▼
┌─────────────────────────┐
│      STORAGE            │
│  (HDFS / S3 / ADLS)     │
│  ▶ QUI STANNO LE RIGHE  │
└─────────────────────────┘
```

---

## CASO REALE: AMAZON (ARCHITETTURA IBRIDA)

Amazon usa entrambi i sistemi:

**MySQL/Aurora** (DB relazionale - OLTP)
- Carrello acquisti: UPDATE immediato (ACID)
- Ordini: INSERT + UPDATE stock (transazione atomica)
- Pagamenti: atomicità critica (tutto o nulla)
- Inventario real-time: decremento immediato
- Latenza: millisecondi

**Hadoop/EMR/S3** (Data lake/warehouse - OLAP)
- Raccomandazioni: miliardi di transazioni storiche analizzate
- Analytics e BI: report su vendite, trend, previsioni
- Click-stream analysis: miliardi di click e navigazioni
- Machine learning: terabyte di dati per addestrare modelli
- Data science: A/B test, segmentazione, pricing
- Latenza: secondi/minuti/ore

**Flusso**:
1. Cliente ordina → **MySQL** registra (real-time, ACID)
2. Di notte → **Sqoop** esporta ordini a **S3** (Ossia datastore distribuito)
3. **EMR**(servizio che gestisce cluster hadoop) elabora milioni di ordini
4. Risultati in **Redshift** (DW) o **S3** (data lake)
5. Analytics, ML e BI leggono da Redshift/S3

**Sintesi**: MySQL = sistema operativo (transazioni live); Hadoop = sistema analitico (big data, ML, BI su dati storici).

---

## RIEPILOGO FINALE

| Componente    | Ruolo                                | Possiede/Governa Dati | Latenza           |
|---------------|--------------------------------------|------------------------|------------------|
| **Storage**   | Memorizza file fisici                | Possiede               | N/A              |
| **Metastore** | Catalogo e governance (tecnica?)     | Governa metadati       | N/A              |
| **Hive**      | DataWhareshouse + query batch        | Governa (non possiede) | Secondi/minuti   |
| **Impala**    | Query interattivo MPP in-memory      | Governa (non possiede) | Secondi/milli    |
| **Sqoop**     | Data transfer DB ↔ Hadoop            | Trasporta (non governa) | Batch           |

**Governance vs Possesso**:
- Lo **storage possiede** fisicamente i dati (file su HDFS/S3/ADLS)
- Il **data warehouse governa** i dati (metadati, schemi, tabelle, sicurezza, processi)
- I **motori SQL leggono/scrivono** i dati secondo il governo del warehouse

**Nota importante:**
- **Data Warehouse** (Hive, Snowflake) = esegue query SQL e governa tecnicamente
- **Data Catalog** (Alation, Collibra) = documenta significato business (NON esegue query)

# Cloudera runtime
Innanzitutto nel contesto IT e Big Data, "runtime" indica l’ambiente software che gestisce l’esecuzione di applicazioni o carichi di lavoro. È lo strato che fornisce le risorse (CPU, memoria, storage, networking) e le funzionalità necessarie affinché i programmi possano funzionare, orchestrando processi, gestione delle risorse e sicurezza.
Cloudera Runtime è l'insieme completo degli strumenti open source per storage distribuito, elaborazione batch/streaming, SQL, NoSQL, ML, ingestion e orchestrazione dati.

Cloudera Runtime comprende:

Storage:
- HDFS (Hadoop Distributed File System)
- Apache Ozone (object store)

Elaborazione Dati:
- Apache Hadoop MapReduce (batch processing)
- Apache Spark (batch + streaming + ML)
- Apache Hive (SQL data warehouse)
- Apache Impala (interactive SQL)
- Apache Pig (data flow scripting)

Streaming e Real-time:
- Apache Kafka (messaging/streaming)
- Apache Flink (stream processing)
- Spark Streaming
- Apache NiFi (data flow automation)

Database NoSQL:
- Apache HBase (wide-column store)
- Apache Kudu (columnar storage engine)

Resource Management:
- Apache YARN (cluster resource manager)

Data Ingestion:
- Apache Sqoop (DB ↔ Hadoop transfer)
- Apache Flume (log aggregation)

Search e Indexing:
Apache Solr (full-text search)
Coordinamento:

Apache ZooKeeper (distributed coordination)
Workflow:

Apache Oozie (job scheduling/orchestration)
Metadata:

Hive Metastore (catalog centrale)
Machine Learning:

Spark MLlib
In sintesi: Cloudera Runtime è l'insieme completo degli strumenti open source per storage distribuito, elaborazione batch/streaming, SQL, NoSQL, ML, ingestion e orchestrazione dati.

Cloudera Runtime può essere concettualmente paragonato a vSphere nel mondo VMware, in quanto rappresenta il livello di runtime che abilita l’esecuzione dei carichi di lavoro. Tuttavia, mentre vSphere opera a livello infrastrutturale come piattaforma di virtualizzazione, Cloudera Runtime opera a livello applicativo come runtime per workload Big Data e analytics, collocandosi sopra lo strato di virtualizzazione o cloud.

# VMware stack
Hardware
↓
vSphere
↓
VM / OS
↓
Applicazioni

# Cloudera stack
Hardware / Cloud
↓
OS / Container / VM
↓
Cloudera Runtime
↓
Spark / Hive / Hadoop / ML
↓
Data workload

| **Dimensione**               | **Cloudera Runtime (CDP)**                             | **vSphere (VMware)**                          |
| ---------------------------- | ------------------------------------------------------ | ----------------------------------------------|
| **Dominio**                  | Data Platform / Big Data / Analytics                   | Virtualizzazione infrastrutturale             |
| **Livello dello stack**      | Applicativo–dati                                       | Infrastrutturale                              |
| **Funzione principale**      | Eseguire workload dati distribuiti                     | Eseguire macchine virtuali                    |
| **Tipo di workload**         | Spark, Hive, Hadoop, Impala, ML                        | VM generiche (applicazioni, database, servizi)|
| **Astrazione fornita**       | Cluster dati e motori di elaborazione                  | CPU, memoria, storage, rete                   |
| **Gestione risorse**         | Scheduling e resource management (es. YARN)            | Scheduling e resource management (DRS)        |
| **Unità di esecuzione**      | Job, query, applicazioni distribuite                   | Macchine virtuali                             | 
| **Dipendenza dall’OS**       | Richiede un OS sottostante (bare metal, VM, container) | Include un hypervisor che sostituisce l’OS host|
| **Rapporto con l’hardware**  | Indiretto                                              | Diretto                                       |
| **Collocazione tipica**      | Sopra VM / container / cloud                           | Direttamente sopra l’hardware                 |
| **Ruolo nella piattaforma**  | Runtime dei dati                                       | Runtime dell’infrastruttura                   |
| **Ambiente di riferimento**  | CDP (Public Cloud / Data Center)                       | Data center virtualizzato                     |
| **Esempi di componenti**     | Hadoop, Spark, Hive, Impala                            | ESXi, vMotion, HA, DRS                        |
| **Obiettivo architetturale** | Standardizzare l’elaborazione dei dati                 | Standardizzare l’uso dell’hardware            |
| **Governance e sicurezza**   | Integrata tramite SDX (Ranger, Atlas)                  | Demandata a strumenti esterni o superiori     |
| **Relazione reciproca**      | Può girare sopra vSphere                               | Può ospitare Cloudera Runtime                 |

## CDP Public Cloud vs CDP Private Cloud

| **Dimensione**                | **CDP Public Cloud**                                    | **CDP Private Cloud**                                  |
| ----------------------------- | ------------------------------------------------------- | ------------------------------------------------------ |
| **Infrastruttura**            | AWS, Azure, Google Cloud (gestita dal provider)         | On-premise o cloud privato (gestita dall'azienda)      |
| **Deployment**                | SaaS-like, provisioning automatico                      | Installazione manuale su cluster dedicati             |
| **Gestione cluster**          | Automatica (Cloudera gestisce upgrade e patching)       | Manuale (IT interno gestisce tutto)                    |
| **Scalabilità**               | Elastica, scale up/down on-demand                       | Limitata dalla capacità fisica del data center         |
| **Costi**                     | Pay-as-you-go (consumo effettivo)                       | CapEx: hardware + licenze + manutenzione               |
| **Time-to-value**             | Rapido (minuti/ore)                                     | Lento (settimane/mesi per setup)                       |
| **Manutenzione**              | Cloudera gestisce infrastruttura e runtime              | IT interno gestisce hardware, OS, runtime              |
| **Sicurezza dati**            | Multi-tenant, dati su cloud pubblico (cifratura)        | Single-tenant, dati rimangono on-premise               |
| **Compliance**                | Conforme a standard cloud (SOC2, ISO, GDPR)             | Controllo totale per requisiti specifici (HIPAA, PCI)  |
| **Networking**                | VPC, connessioni cloud-native                           | Rete aziendale interna                                 |
| **Disaster Recovery**         | Nativo cloud (multi-region, backup automatici)          | Richiede setup dedicato (backup, replica)              |
| **Flessibilità hardware**     | Provider cloud decide (preset configurazioni)           | Controllo totale su hardware e configurazione          |
| **Workload ideali**           | Analytics, ML, BI con carichi variabili                 | Workload critici, dati sensibili, compliance rigoroso  |
| **Esempi casi d'uso**         | Startup, progetti sperimentali, burst capacity          | Banche, sanità, governativi, legacy integration        |
| **Dipendenza vendor**         | Forte (Cloudera + cloud provider)                       | Moderata (Cloudera software, hardware proprio)         |
| **Aggiornamenti**             | Automatici, gestiti da Cloudera                         | Pianificati e applicati dall'IT interno                |
| **Costo prevedibilità**       | Variabile (dipende dall'uso)                            | Fisso (hardware ammortizzato)                          |
| **Skills richiesti**          | Cloud-native, meno sysadmin                             | Sysadmin, networking, storage management               |

**Scelta strategica:**
- **Public Cloud**: velocità, elasticità, riduzione complessità operativa
- **Private Cloud**: controllo, sicurezza, compliance, integrazione legacy


CDP Public Cloud pone l’accento sull’utilizzo dell’object store del provider cloud, invece di HDFS come avveniva in CDH e HDP. Questo determina una separazione tra calcolo e storage, consentendo a ciascun workload di disporre della propria capacità di elaborazione pur continuando ad accedere agli stessi dati sottostanti.

Poiché le implementazioni di CDH e HDP collocano insieme storage e calcolo, esse non sono adatte a workload transitori. In CDP Public Cloud (così come in CDP Private Cloud), gli amministratori possono registrare tutti gli ambienti di cui hanno bisogno.