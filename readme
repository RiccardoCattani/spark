# Copia il file nella tua home sul server remoto
scp /mnt/nvme_storage/download/bigbasket_products.csv riccardo@192.168.1.229:~/tmp

# lanciare in bash sul server remoto (Né in hive né in spark-shell funziona il caricamento del file csv)
# Questo comando esegue un programma scala che usa spark per leggere il file csv
sbt run

# Spostamento file
mv /home/riccardo/tmp/bigbasket_products.csv /home/riccardo/datasets/bigbasket_products.csv

# inserimento variabili d'ambiente in maniera permanente
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk
export PATH=$JAVA_HOME/bin:$PATH

# rimuovi le modifiche alla cartella target
git checkout -- target/

# ignora la cartella target con tutti i suoi contenuti in git (è la cartella di output di sbt, dove vengono messi i file compilati)
echo "target/" >> .gitignore
git rm -r --cached target/
git add .gitignore
git commit -m "Ignora la cartella target"

# Comando per eseguire il jar  serve per eseguire il tuo programma Spark (compilato nel file JAR) fuori da spark-shell o da SBT, direttamente tramite Spark 
# Va eseguito con spark-submit nella bash fuori da spark o sbt
# Consente di lanciare spak come applicazione standalone (Indipendentemente da SBT o spark-shell)
spark-submit --class sparkPratica.rdd_Oggetto --master local[*] /home/riccardo/Documenti/spark/target/scala-2.12/sparkpractice_2.12-0.1.jar

# Verifica che il file sia stato caricato correttamente in HDFS
hdfs dfs -ls user/cloudera/bigbasket

# Differenza tra sbt run e spark-submit
La differenza principale tra sbt run e spark-submit è:

- sbt run
Compila ed esegue il programma Scala direttamente dal progetto SBT.
Utile in fase di sviluppo e test locale.
Gestisce automaticamente le dipendenze definite in build.sbt.
Non richiede la creazione di un file JAR separato.
Va usato dalla cartella del progetto dove c’è build.sbt.

- spark-submit
Esegue un file JAR già compilato come applicazione Spark indipendente.
È il modo standard per lanciare job Spark in produzione o su cluster.
Può essere usato su qualsiasi nodo Spark (anche senza SBT installato).
Richiede che tu abbia già creato il JAR (sbt package).

In sintesi:

sbt run: sviluppo locale, esecuzione diretta dal codice sorgente.
spark-submit: esecuzione di un'applicazione Spark già compilata, usato in produzione o su cluster.

# creo ora questo src/main/scala/sparkPractise/obj_Logs.scala

package sparkPractise
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object obj_Logs {

    def main(arg:Array[String]):Unit=
    {
        val conf=new SparkConf().setAppName("TestLog").setMaster("local[*]")
        val sc=new SparkContext(conf)
        sc.setLogLevel("Error")

        val inputRDD=sc.textFile("file:///C:/data/test_log.txt")
        inputRDD.foreach(println)

    }
}

# verificare sempre quale versione di java è in uso e quale è richiesta 

java -version

# per metterla temporaneamente a livello di sessione bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export PATH=$JAVA_HOME/bin:$PATH

# per metterla in maniera fissa

echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc

# Common RDD actions and transformations

Le "common RDD actions" in Apache Spark sono operazioni che, quando invocate su un RDD (Resilient Distributed Dataset), fanno sì che Spark esegua effettivamente il calcolo e restituisca un risultato al driver o produca un output esterno. Le azioni sono diverse dalle "transformations", che invece creano nuovi RDD senza eseguire subito il calcolo.

Ecco alcune delle azioni più comuni sugli RDD:

collect(): restituisce tutti gli elementi dell’RDD come array al driver.
count(): restituisce il numero di elementi nell’RDD.
take(n): restituisce i primi n elementi dell’RDD.
reduce(func): aggrega gli elementi dell’RDD usando una funzione binaria.
first(): restituisce il primo elemento dell’RDD.
countByValue(): restituisce una mappa con la frequenza di ciascun valore.
saveAsTextFile(path): salva il contenuto dell’RDD come file di testo.
foreach(func): applica una funzione a ciascun elemento dell’RDD (effetto collaterale).

Queste azioni sono fondamentali per ottenere risultati concreti dal calcolo distribuito di Spark.

# Differenza tra DataFrame e RDD

**RDD (Resilient Distributed Dataset):**
- Collezione distribuita di oggetti, immutabile e tipizzata, su cui si possono applicare trasformazioni (map, filter, flatMap, ecc.) e azioni (count, collect, foreach, ecc.).
- Offre controllo di basso livello su partizionamento, caching e operazioni custom.
- Non ha schema: ogni elemento è un oggetto Scala/Java (es. String, Array, case class).
- Meno ottimizzato: non sfrutta il motore Catalyst e le ottimizzazioni SQL di Spark.
- Utile per operazioni non tabellari, manipolazioni complesse o dati non strutturati.

**DataFrame:**
- Collezione distribuita di dati organizzati in colonne (simile a una tabella SQL o a un DataFrame Pandas).
- Ha uno schema (nomi e tipi delle colonne) e supporta operazioni SQL-like (select, filter, groupBy, join, ecc.).
- Più ottimizzato: sfrutta il motore Catalyst per ottimizzazioni query e Tungsten per la gestione efficiente della memoria.
- Supporta API SQL, DataFrame e Dataset, e può essere usato con Spark SQL.
- Consigliato per analisi tabellari, aggregazioni, join e quando serve performance.

**In sintesi:**
- Usa RDD per operazioni di basso livello, dati non strutturati o manipolazioni custom.
- Usa DataFrame per analisi tabellari, query SQL, aggregazioni e performance migliori.

**Esempio pratico:**

// RDD
val rdd = sc.textFile("India.txt")
val hindiStates = rdd.filter(line => line.split(",").last.trim == "Hindi")
hindiStates.collect().foreach(println)

// DataFrame
import spark.implicits._
val df = spark.read.option("delimiter", ",").csv("India.txt").toDF("Stato", "Capitale", "Lingua")
df.filter($"Lingua" === "Hindi").show()


